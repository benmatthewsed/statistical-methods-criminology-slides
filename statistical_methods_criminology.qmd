---
title: "Statistical Methods for Criminology"
title-slide-attributes:
    data-background-color: "#006938"
author: "Ben Matthews | University of Stirling"
bibliography: references.bib
execute: 
  echo: true
format:
  revealjs:
    theme: [clean.scss]
    smaller: true
    scrollable: true
---

##

![](https://c2.thirdlight.com/file/8/Tb2SsM4TXyBWQHTblABTsjunye/primary-logo.png){width=80%}

![](https://www.ncrm.ac.uk/site-resources/images/webpage-meta-image.jpg)




## Welcome!

- Who we are
- Who you are:
  - Your __research interests__
  - __Why__ this course?
  - What do you want to __achieve__?


## Housekeeping

- [Code of conduct](https://github.com/benmatthewsed/statistical-methods-criminology-slides/blob/master/resources/ncrm-general-code-of-conduct.pdf)
- __Ask questions whenever__ :thumbsup: You can do this through the Zoom chat facility __{demo this now}__
- We've structured the sessions with regular breaks, but __if you need to leave just leave__!
- Materials will live online ([slides](https://benmatthewsed.github.io/statistical-methods-criminology-slides/statistical_methods_criminology.html#/title-slide),) so you can access them any time


---

## Learning outcomes

By the end of the workshop you will:

- Understand the main forms of criminological data 
- Understand how data about crime and victimization can be understood using the General Linear Model (GLM)
- Be aware of issues that can arise in interpreting GLM coefficients due to measurement error, selection bias and omitted variables
- Be aware of ways to transform GLM coefficients into meaningful Derived Quantities of Interest (DQI)
- Understand ethical issues around the discussion of quantitative results


---

## Course outline

![](resources/course-outline.jpg)


## Course schedule

- 10:00 – Welcome
- 10:15 – Data provenance: Types of criminological data
- 11:15 – Break
- 11:30 – Modelling: Telling stories with the general linear model
- 12:30 – Lunch
- 13:30 – Q&A
- 13:40 – Sensitivity analysis and critiquing results: Measurement error, selection and - confounding in crime data
- 14:40 – Break
- 14:55 – Simulation methods and translating results: Making model coefficients meaningful with simulation
- 15:55 – Short break
- 16:05 – Discussing results and ethics
- 16:45 – Recap and Q&A
- 17:00 – Depart


---

## Session structure

- In each session I'll present for the first half(ish)
- Then there will be some exercises and group discussion for the second half(ish)

## Practical sessions

- You can follow along with the materials in a local installation of `R` and `RStudio` on your own computer
- You can find the course materials at https://github.com/benmatthewsed/statistical-methods-criminology-slides/
- If you don't have an `R` installation, you can follow along in your browser via posit.cloud (free account required) [Ben post the link in the chat now]
- Or you can use the PDFs of the results rather than run them yourself



# About this workshop {background-color="#006938"}

---

## What this workshop is not

- This workshop is not an in-depth guide on how to:
  - fit a particular statistical model, 
  - select which kind of statistical model you should fit
  - use particular statistical software
- These are important topics! And there are a [wealth of training courses](https://www.ncrm.ac.uk/training/) covering them

## What this workshop is not (continued)

- This workshop is also not a comprehensive guide to every issue that can arise analysing criminological data
- Criminology as a 'rendezvous discipline' as David Downes once said[^1]
- You could plausibly be analysing a [randomized control trial](https://link.springer.com/journal/11292), or [spatial data](https://www.annualreviews.org/content/journals/10.1146/annurev-criminol-011419-041423), or [attitudinal questions from a survey](https://journals.sagepub.com/doi/abs/10.1177/1748895817721273) - all of which have specific considerations for statistical analysis - and still be doing criminology

[^1]: As cited in @youngPraiseDangerousThoughts2003


## What this workshop _is_

- Instead, this workshop will give an introduction to a set of issues that are (I hope) general to most (or maybe all) analysis of criminological data
- Some of these issues do not have obvious solutions - but at the end of the workshop you will at least be able to describe and acknowledge them, and the possible impacts they may have on inferences from criminological data
- I will also signpost where you can go to find out more about each of these issues so you can keep exploring those that are most relevant to you


## Course outline

![](resources/course-outline.jpg)



## Statistical analysis and stories

- This workshop is about statistical methods for criminology
- This workshop is _also_ about stories. Specifically:
  - The story of how the data in your spreadsheet came to exist (sometimes called [data provenance](https://betanalpha.github.io/assets/chapters_html/tree_diameter_growth_analysis.html#sec:provenance))
  - The story you tell about these data based on a statistical analysis (i.e. your results)
- I want to convince you that the first of these two stories should filter through into every decision you make during analysis, which determines the story you can tell about your results
- You need to know how your data came about to be able to analyse it properly




# Session One: Types of criminological data{background-color="#006938"}


## Types of criminological data

- A rendesvouz discipline - varied questions, varied approaches to answering them using varied data
- Criminological data could be:
  - Administrative data from the justice system (police, courts, prisons, probation... etc)
  - Secondary survey data (e.g. victimization, offending, fear of crime... etc)
  - Newspaper reports (e.g. collations of stories on police use of force or homicides) [@roussellDarkFootprintState2022]
  - Social media data (e.g. fear of crime) [@solymosiCrowdsourcingSubjectivePerceptions2018]
  - 'Digital trace' data from darknet drugs transactions [@cunliffeNonmedicalPrescriptionPsychiatric2019]
  - ...

---

## Key types of criminological data

- Here we focus on administrative data (mostly police recorded crime data and convictions data) and victimization surveys as the most common forms of criminological data
- If you are interested in another form of data, please do mention this in the discussion sessions!

## Data provenance as the story of your data

- Data provenance: how the specific numbers in your spreadsheet came to be in your spreadsheet
- Sometimes well documented (in the case of e.g. national surveys)
- Sometimes not so well documented...

## Understanding data provenance

- [@8347e95e012d4212a5ee429a18ee592e, p228-229] recommend six questions to ask about a dataset which can help understand the data's provenance:
  - Why has the data been collected (and collected in this way)?
  - How has the data been collected and/or by whom or by what?
  - What/who is included and what/who is excluded?
  - What is the context for the data collection (routine activity, bespoke intervention, to   meet a target)?
  - Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at   its present form?
  - What are the relevant definitions and concepts that govern the data/data collection?
- We're going to spend the rest of this session looking at some examples of crime data, using these questions to understand data provenance in these examples, and then thinking about the implications for how we might analyse these data


# Data provenance
# Example One: How does an event become a crime statistic?


## How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![How a crime is recorded flowchart](resources/sg-recorded-crime-01.png)



:::

::: {.column width="34%"}
Like this!

Source: [@justiceanalyticalservicesUserGuideRecorded2016]

:::

::::


## Example One: How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![How a crime is recorded flowchart](resources/sg-recorded-crime-02.png)



:::

::: {.column width="34%"}
- Reporting

Source: [@justiceanalyticalservicesUserGuideRecorded2016]
:::

::::



## Example One: How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![How a crime is recorded flowchart](resources/sg-recorded-crime-03.png)

:::

::: {.column width="34%"}
- Reporting
- Fact-checking/investigation

Source: [@justiceanalyticalservicesUserGuideRecorded2016]

:::

::::



## Example One: How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![How a crime is recorded flowchart](resources/sg-recorded-crime-04.png)

:::

::: {.column width="34%"}
- Reporting
- Fact-checking/investigation
- Discretion/judgement

Source: [@justiceanalyticalservicesUserGuideRecorded2016]

:::

::::


## Example One: How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![How a crime is recorded flowchart](resources/sg-recorded-crime-05.png)

:::

::: {.column width="34%"}
- Reporting
- Fact-checking/investigation
- Discretion/judgement
- Applying crime counting rules

Source: [@justiceanalyticalservicesUserGuideRecorded2016]

:::

::::

## Understanding data provenance: police recorded crime

- This is quite complicated!
- (At least the process is known and documented though!)
- Now we can answer some of our questions from before

## Data provenance: recorded crime data
- Why has the data been collected (and collected in this way)?
    - Standard operating practice?
- How has the data been collected and/or by whom or by what?
    - See previous slides!
- What/who is included and what/who is excluded?
    - Excludes: crimes not reported and for which there was insufficient evidence
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - It Depends? National statistics are routine collections, [there are other data though](https://www.law.ed.ac.uk/sites/default/files/2022-08/FPN%204th%20report%20-%20FINAL.pdf)
    
    
## Questions to ask any type of data (continued)
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - If it's data from the recorded crime data bulletin - yes, cleaned and standardized
- What are the relevant definitions and concepts that govern the data/data collection?"
  - The current [Scottish Crime Recording Standard](https://www.gov.scot/publications/scottish-crime-recording-standard-crime-recording-counting-rules-2/) is 550 pages long (!). I have not read it...
    - Of course, these [aren't the same crime recording rules as in England and Wales...](https://reshare.ukdataservice.ac.uk/852854/)

---

## Some implications of data provenance

- Requirements to report on crime levels mean there are incentives to reduce figures?
    - There is a [complicated recent history of recorded crime quality in England and Wales](https://osr.statisticsauthority.gov.uk/publication/the-quality-of-police-recorded-crime-statistics-for-england-and-wales/)
- Excludes incidents not reported (see ['dark figure of crime'](https://www.adruk.org/our-mission/our-impact/measuring-the-dark-figure-of-crime-479/) - the gap between police recorded crime and estimates from victimization surveys)
- Recorded crime figures represent (unknown? unknowable?) mix of 'behavioural' and 'system' effects [@kitsuseNoteUsesOfficial1963]
  - For example, urban areas have high police recorded crime rates because [@simesPolicingPunishmentPlace2023]...
      - that's where crime is?
      - that's where poverty and disadvantage are?
      - that's where (disproportionate) surveillance and punishment are?


## Further implications of data provenance for analysis

- If we are working with recorded crime data, we have data on all[^2] crimes reported to the police for a particular geography over a particular time period

[^2]: Notwithstanding the other stuff we spoke about before

# Is this a 'sample' of data from a wider poopulation?

## Population or 'super-population'?
- _WARNING_ statistical jargon ahead!
- There is no right or wrong answer here - it depends on what you want to use the data to find out
- _Population approach_ [@verlaan2024use]: if you have police recorded crime data for Scotland in 2023 the data you have is all you could ever have.
  - It is not a sample
  - All you need are descriptive statistics
- _Superpopulation approach_ [@verlaan2024use]: you can think of Scotland in 2023 as 'drawn' from from the population of the UK, or Europe, or the whole planet
  - It is a kind of sample, because I want to use it to generalize to this wider geographical population
  - Or data from Scotland from 2023 may be seen as a sample from Scotland from 2023 and 2024, or 2023-2030 and so on...

## Population or 'super-population'?

- There is no right or wrong answer... but you answer will (or should at least) determine what subsequent analysis you do
- If you take the population view you don't need to fit statistical models to generalize from the data you observe to a wider population because *there is no wider population*
- If you just want to know how many housebreaking incidents were recorded by the Police in Shetland in March 2024 you don't need a statistical model
- However, often we have more analytically ambitious aims than this...

## Personally I prefer a 'superpopulation' approach

- It can be useful to report confidence intervals even when describing statistics from the full population - especially if the results are to be used to make predictions or inform policy [@d.redelingsWhyConfidenceIntervals2012]
- For example, if we want to make comparisons between different areas, places different sized populations (e.g. local authorities) have different levels of variability [@spiegelhalterFunnelPlotsComparing2005]. Crime rates in small places will be more volatile - and so have more uncertainty - year-on-year than those from large areas
- "In reality, all systems within which ... local authorities operate, no matter how stable, will produce variable outcomes in the normal run of events. In particular, outcomes in local authorities with smaller sized populations tend to vary more than those in local authorities with larger populations. The question we need to answer is therefore: Is the observed variation more or less than we would normally expect?" [@scottishgovernmentReconvictionRatesScotland2021]


## Dangers of inferential statistics with recorded crime data

- However, @verlaan2024use outline two compelling problems when using inferential statistics and statistical significance testing with recorded crime data
1. A statistically significant result can wrongly leads people to assume that their results are generalizable
  - But there might be other reasons what you see in your dataset about crime in Scotland in 2023 might not apply to Scotland in 2024, or Germany in 2023... etc
2. It can lead people to undervalue actual observed differences in their data
  - For example, researchers may underplay or even deny that an association between two variables exists in their dataset if it is not statistically significant (It's a common fallacy that a non-statistically significant result means that a finding is not 'true')

## Recap: Data provenance of police recorded crime

- Police recorded crime data are the product of a complicated operational system
- It is likely an undercount of the amount of crime 'out there' in society, but is a full record of all crimes recorded by the police
- There are different perspectives about whether this is a 'sample' of data
- Things like confidence intervals (more associated with sample data) can still be useful

# Example Two: Understanding victimization surveys

## Why victimization surveys?

- In response to known issues with measuring levels of crime with administrative data, since the 1980s criminologists in some parts of the world have been surveying the public to ask about their levels of victimization [@maguireCrimeDataCriminal2017]
- Examples include:
  - [Scottish Crime and Justice Survey(SCJS)](https://www.gov.scot/collections/scottish-crime-and-justice-survey/)
  - [Crime Survey for England and Wales](https://www.crimesurvey.co.uk/en/index.html)
  - Equivalents in other countries, primarily in Western Europe, North America and Australasia
  - Also smaller geographical scale surveys such as [The Mayor's Office for Policing And Crime (MOPAC) Survey](https://data.london.gov.uk/dataset/mopac-surveys) in London
- We'll focus on the SCJS

---

## Data provenance: victimization surveys

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of government reporting on experiences and attitudes of criminal justice
- How has the data been collected and/or by whom or by what?
    - Collected by professional survey agencies
- What/who is included and what/who is excluded?
    - Random sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)
  - For those who agree, one adult (age 16 or over) in each household is randomly selected for interview​
  

## Answering our key questions

- What is the context for the data collection?
    - Government-commissioned survey
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - When it gets to you it has usually been validated. Some information is also restricted, such as 'capping' counts of victimization - we'll talk about this later...
- What are the relevant definitions and concepts that govern the data/data collection?"
  - You can read the question wording online! [But... the questions included can changes over time](https://www.sccjr.ac.uk/wp-content/uploads/2024/02/group-two_hackathon-output.pdf)


## Implications of data provenance

- Relatively small[^5] sample size means they are good for measuring common crimes, bad for measuring rare crimes
- It also may mean they disaggregation by geography or demography might not be possible or ethical
- The sampling frame means that, *by design*, the survey doesn't cover:​
  - Children​
  - Homeless people​
  - People living in communal establishments (e.g. students, people in prison)
  - Also in practice may exclude those in 'fragile, disjointed households' [@carr-hillMissingMillionsMeasuring2013]
  
[^5]: At least compared to the number of crimes recorded by the police

## Implications of data provenance

- For SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population...”
- However, “... domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime” [@scottishwomensaidResponseConsultationScottish2021]


  
## Analysing survey data

- Survey data are definitely a sample, so we almost always want to perform inferential statistics or adjust the data in the spreadsheet to better match the target population
- Surveys are very rarely random samples; most come with weights - use the weights for descriptives to gross up to national populations!
- Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model - it may depend on the specifics of your survey
- But, weights will only help adjust the data you see _towards the target population in the survey design frame_. Weighting cannot adjust for populations who are excluded from the survey by design.

## Comparing police recorded crime and victimization survey data

![Stengths of recorded crime and victimization surveys](resources/sg-strengths-limitations-01.png)
Source: [@justiceanalyticalservicesUserGuideRecorded2016, p57]

## Comparing police recorded crime and victimization survey data

![Stengths of recorded crime and victimization surveys](resources/sg-strengths-limitations-02.png)

Source: [@justiceanalyticalservicesUserGuideRecorded2016, p57]

--- 

# Recap



## Recap

- Data provenance influences what a dataset can tell us and how we should analyse it
- Recorded crime and victimization surveys have strengths and limitations that stem from the provenance of these data types



# Practical

- See description of [Scottish Offenders Index here](https://github.com/benmatthewsed/statistical-methods-criminology-slides/blob/master/practical-01.md)
- Answer our six questions
- We'll meet back at [time] to feed back


# Break {background-color="#666666"}


# The general linear model  {background-color="#006938"}


---



## Disclaimer One

- If you are taking this course I assume that you have some familiarity with linear/regression models of some description
- This session will give only a brief refresher
- Mostly we will focus on matching your statistical model to the story of your data

---


## Disclaimer Two

- You could spend months and months studying [Generalized Linear Models (GLM)](https://sscc.wisc.edu/sscc/pubs/glm-r/) and their various extensions
- Most methods courses and workshops are focused on a particular statistical model (though I haven't checked)
- But as we also discussed earlier, these can be quite application specific (developmental models for conviction trajectories; spatial models for geographical analysis; time series models for crime trends etc.)
- Some specific flavours of model that we *don't* cover, but which may be useful for your own work include:
  - [Mixed/multilevel/hierarchical models](https://www.cmm.bris.ac.uk/lemma/)
  - [Generalized Additive models (GAMs)](https://noamross.github.io/gams-in-r-course/)
  - ['Bespoke' models](https://betanalpha.github.io/assets/case_studies/generative_modeling.html)


## Disclaimer Three

- A statistical model is only ever the map and never the territory
- The statistical models are 'small worlds' of their own - distinct from the 'large world' that we actually live in
- Our spreadsheets and the coefficients produced by our models necessarily omit some of the complexity of the large world - [this is a good thing](https://kwarc.info/teaching/TDM/Borges.pdf)!
- But be warned, using the small world to understand the large world is fraught with challenges (we'll talk about this more later)


## Anatomy of a GLM

- GLMs are mathematical ways to summarise the relationship between an outcome you care about and a set of predictors
- They are _extremely_ useful and very widely used statistical models to answer questions of the kind 'if I have more of X will I also get more of Y'?
- All GLMs have the following:
  - A systematic component, which describes how the predictors relate to the outcome
  - A stochastic/random component, which describes the [probability distribution](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/probability-distribution/) of the outcome
  - A [link function](https://www.statisticshowto.com/link-function/) that connects the systematic and random component
- _WARNING_ there are equations in the next few slides (sorry)


## The linear model

For the standard linear regression model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x),
\end{align*}
$$

- Where:
  - $y_i$ is the outcome
  - ${Normal}$ is the random component, with a mean ($\theta_i$) and a variance ($\sigma$)
  - $\theta_i$ is the systematic component, made up of $\alpha + \beta (x)$ - the model coefficients
  - $Identity()$ is the link function (in this case, the function doesn't do anything!)
  
## Linear regression


For the standard linear regression model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x),
\end{align*}
$$

- We then get a model of our outcome[^7] which we can interpret, often by looking at the values of $\beta (x)$ - how much the outcome changes for a one-unit change in our predictor ($x$) variable(s)
- One important thing for us is that this definition means that our outcome _can take any value from $-\infty$ to $\infty$_. It is not constrained in any way.

[^7]: Strictly speaking, the [expected value of the outcome](https://www.statisticshowto.com/link-function/), but let's not worry about that now
  
  
## GLMs and data provenance

- But let's think back to the two data provenance case studies we discussion in the last session
- Police recorded crime and responses to victimization surveys _are_ constrained: they are both counts of things
- This means that - by definition - they have to be positive. A police force cannot report -3000 crimes for the last month, and a person can't say in response to a survey that they have experienced negative two incidents in the last year

## GLMs, families and probability distributions

- This is where GLMs come in - they allow using different probability distributions for your outcome to ensure that the model can only predict, say, positive counts
- There are other common probability distributions for things like data which reflect whether something happened or it didn't
- However, the model coefficients themselves still need to be able to go from $-\infty$ to $\infty$_ - which is why we need a link function to map between the outcome and the coefficients
- Here we're going to focus on a common model for count data, but there are [lots of families and link functions out there](https://www.statisticshowto.com/link-function/)

## Poisson model

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$


- Where:
  - $y_i$ is the outcome
  - ${Poisson}$ is the random component, with a single parameter $\lambda$
  - $\lambda$ is the systematic component, made up of $\alpha + \beta (x)$ - the model coefficients
  - $log()$ is the link function (in this case, the function takes the [logarithm](https://en.wikipedia.org/wiki/Logarithm) of the coefficients to estimate $\lambda$)



## Poisson model

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$


- For reasons we won't go into now, $\lambda$ has to be positive
- This is we why need to take the $log()$ of the coefficients - the $log()$ function will convert numbers from $-\infty$ to $\infty$_ to numbers between $0$ and $\infty$_
- Also note that now the Poisson distribution only has one parameter (lambda; $\lambda$), unlike with linear regression which has a mean and a variance. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).


## Why bother with all this?

- To demonstrate why people go through all this rigmarole let's walk through an example of using standard linear regression with count data
- In the next slide we simulate some data from a count distribution, and then fit first a linear regression model with only an Intercept ($\alpha$), and then a Poission model
- The data here is fake, but is not implausible if we were interested in, say, the number of violence incidents experienced in a given years amongst the respondents to a small victimization survey

## Why bother with all this?

```{r}
library(dplyr)

set.seed(12346)

n_draws <- 1000

dat <- 
data.frame(
  y = rpois(n = n_draws,
            lambda = 0.001) # draw from poisson distribution with mean 0.001
)

dat |> 
  count(y)

```

## Why bother with all this?

```{r}

lm(y ~ 1, data = dat) |> # fit intercept-only model with normal outcome
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error)


glm(y ~ 1, # fit intercept only model with poisson outcome
    family = "poisson",
    data = dat) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low))

```

Here the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive).


## Part Two: Interpreting coefficients

- If you notice, for the Poisson model I had to use the $exp()$ function before interpreting the model coefficients
- This is because the model uses the $log()$ link function - $exp()$ is the reverse of $log()$
- For the model's Intercept (the value of the outcome when all the independent variables have value zero), the exponentiated value is the mean of the outcome
- If we add predictors into the model, we can still $exp()$ the coefficients and then interpret the exponentiated coefficients as the _percentage change_ in the outcome for a one-unit increase in the independent variable


## Interpreting coefficients


```{r}

simd <- readRDS(url("https://github.com/benmatthewsed/statistical-methods-criminology-slides/raw/master/resources/simd_crime_sim.rds"))

glm(crime_integer_sim ~ overcrowded_rate, # % living in overcrowded housing
    family = "poisson",
    data = simd) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         conf_upp = estimate + 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low),
         exp_conf_upp = exp(conf_upp))

```

- Going from 0% overcrowded to 100% would be associated with a 280% increase in crime in a neighbourhood (!)
- Need to remember the ["fundamental statisical question": compared with what?](https://notstatschat.rbind.io/2022/01/02/per-capita-in-mice/)
- (NB How realistic to you think this is?)


## GLMs aren't all good news

- Using a GLM for count data resolves the problem of getting negative estimates and confidence intervals
- However, interpreting the coefficients gets more complicated
- And in the specific case of Poisson regression there is another specific problem...


## Quasi-poisson models and over dispersion

- This is quite technical, so I will be brief!
- The Poisson distribution assumes mean and variance are the same (in that there is only one term in the model, $\lambda$, which describes both the average and the variability around the average)
- This is quite a brittle assumption though, and there's no reason that real-life data has to obey it
- The problem is that if this assumption is not met and your data are [over-dispersed](https://online.stat.psu.edu/stat504/book/export/html/779) all your statistical tests will be over-confident, and you will find statistically significant results when you shouldn't
- One fix is to use a quasipoisson model, which does not have this assumption. This is what  @walbyViolentCrimeIncreasing2016 used when modelling counts of victimization data, although [other options are available](https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/)

# Recap

## Recap
- GLMs are common statistical models used in criminology
- We should match the type of GLM to the 'story' of our data, commonly (but not exclusively) in criminology this is a count model
- However, these models can be tricky to interpret, and can have other problems too




## Practical

In this practical we'll fit some count models in `R` and interpet the results.

You can [download the `R` script here](https://github.com/benmatthewsed/statistical-methods-criminology-slides/blob/master/practical_02.R)

# Morning recap {background-color="#006938"}

## Morning recap

- The story of how our data came to be influences how we should analyse it
- GLMs are the most popular way to tell stories about the `small world` of our data


# Lunch :sandwich: {background-color="#666666"}

---



# Measurement error, selection and confounding  {background-color="#006938"}


---

## Going from the small world to the large world

- In the last session we talked about GLMs as 'small worlds' which summarise the data in our spreadsheets, and ended by interpreting coefficients from GLMs
- In practice there are lots of reasons we may be skeptical about applying conclusions drawn from the 'small world' of our data and model to the 'large world' we live in
- In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models
- Each is a whole area of research itself, so we only offer a brief overview of each, with links to further reading for those who want to know more.

---

## Are coefficients sufficient?

- Coefficients from GLMs may biased because:
  - our data are inaccurate [(measurement error)](https://www.statisticshowto.com/measurement-error/)
    - omit some people/cases we care about [(selection effects)](https://en.wikipedia.org/wiki/Selection_bias), or;
  - our data omits some variables that we theoretically think are important [(confounding)](https://stats.stackexchange.com/a/496387)
- These are problems that are _hard, if not impossible, to solve based on the data we observe alone_
- So in this session we focus on sketching out the issues and pointing to resources to find out more information


# Part One: Measurement error {background-color="#77BF22"}


## Measurement error

- [Measurement error](https://www.statisticshowto.com/measurement-error/) is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet
- In practice we know criminological data are likely to be measured with some degree of error
- In this section we will walk through two examples of measurement error in crime data, again focusing on police recorded crime and victimization surveys


---

## Example One: Measurement error in recorded crime

-   From our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data [@pina-sanchezImpactMeasurementError2022]
- As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes... and so on
-   So if we want to know, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an *under* count

## Measurement error in recorded crime

- Measurement error in recorded crime is a particular structure: it is _proportional to the amount of crime_ [@pina-sanchezImpactMeasurementError2022]
- There is a larger gap between the amount of crime recorded and 'true' crime levels in areas with large numbers of recorded crimes compared to areas with very low recorded crime
  - The count of crimes recorded in Glasgow will be more of an undercount of 'true' crime in Glasgow than the count of crimes recorded in Shetland will be an undercount of 'true' crime in Shetland


## Measurement error in recorded crime

![Comparison of property crime and homicide rates using data from the police, a victimisation survey (CSEW) and vital statistics (NCHS), Figure 2 in Pina-Sanchez et al. 2022](https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10940-022-09557-6/MediaObjects/10940_2022_9557_Fig2_HTML.png)


## `rcme`

- @pina-sanchezImpactMeasurementError2022 propose some solutions for common forms of statistical analysis using recorded crime
- They also provide an [`R` package `rcme`](https://github.com/alex-cernat/rcme) which implements some of these methods
- If you are interested fitting statitsical models to police recorded crime data I would strongly recommend checking this out



---

## Example Two: Measurement error in victimizaton survey data

-   Historically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete:
- In 2019, ONS said 

> "Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way."

[@officefornationalstatisticsImprovingVictimisationEstimates2019]

## Capping and time trends

- The problem is that a small number of people experience a lot of crime, and the estimate of overall crime in the survey is very sensitive to the number of very frequent victims who are sampled
- And because these very frequent victims are rare in the general population, the number included a national survey fluctuates year-to-year


## Capping and measurement error

- However, capping also means that people who experienced more than five incidents of a particular 'series' crime type did not have their data accurately recorded
-   This was particularly important for women's reporting of violent victimization [@walbyViolentCrimeIncreasing2016]
- Women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year
- ONS [now caps incident estimates at the 98th percentile of the distribution, rather than at five per series, and makes uncapped data available to estimates volatility over time](https://www.ons.gov.uk/releases/crimeinenglandandwalesyearendingmarch2023)
- After analysis to assess the effect of capping, SCJS retained the cap at five [@scottishgovernmentMethodologicalNoteCalculating2019]


## Adventures in measurement error

- The two examples we've looked at have focused on measurement error in two specific applications
- In general it's hard to predict the impact measurement error may have on any particular analysis
- If you have error in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results [(meaning that they are less likely to be statistically significant)](https://stats.stackexchange.com/a/129993)
- Measurement error in a key independent variable may bias your regression coefficients *downwards*, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed [(sometimes called 'regression dilution')](https://en.wikipedia.org/wiki/Regression_dilution)...
- But if there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to 'under controlling' - and finding statistically significant coefficients where there are none [@fewellImpactResidualUnmeasured2007]
- So it's quite context dependent - so you need to know the likely source of error in your specific dataset




# Part Two: Selection effects

## Selection effects

- Selection effects (or selection bias) arises when there are people who we would have liked to observe in our study but we don't observe them, and this lack of observation is related to their characteristics [@greenlandSensitivityAnalysisBias2014]
- It's a kind of non-representativeness of the data we have, compared to the 'real world'
- I like to think of this as selection bias affecting the rows of our dataset - there are some rows that are missing that we would like to see
- In this section we'll focus on an example of identifying selection bias in police data

## Example: Selection bias in police administrative data

- [@knoxAdministrativeRecordsMask2020] wanted to assess whether police were biased in their interactions with the public based on the ethnicity of the person they were interacting with
- They had data on people who had been stopped by the police - but they can't just fit a statistical model to these data to see if people from minority ethnic backgrounds were more likely to be stopped
- The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased” [@knoxAdministrativeRecordsMask2020]

## Example: Selection bias in police administrative data

- To just rely on the observed data, @knoxAdministrativeRecordsMask2020 would need to know the numbers of people _who were observed by police but not stopped_, in order to calculate the probabilities of selection into the stop dataset
- If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop
- However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset *would* know this
- “when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.” [@@knoxAdministrativeRecordsMask2020]

## Uh oh

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

## What can we do?

- @knoxAdministrativeRecordsMask2020a suggest some technical fixes in their specific case, but emphasise that *there is no general solution* that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records
- The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader

## What can we do?
- Another possibility suggested by @gaeblerCausalFrameworkObservational2022 is that it may be possible to identify some aspects of discrimination in administrative data
- This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?)
- This basically involves changing the question you are asking - which might be fine, but is not a very satisfactory solution

---


# Part Three: Confounding

## Unmeasured confounding

- Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we _don't have a measure for this in our data_
- I like to think of this as affecting the _columns_ of our dataset: there are variables we wish we had, but in practice we don't
- This issue is more commonly discussed in epidemiology and health fields as opposed to social sciences [@dingFirstCourseCausal2023]


## An example of unmeasured confounding
- Famous example of smoking and lung cancer
  - RA Fisher argued that the strong association between smoking and lung cancer "could both be influenced by a [unmeasured confounder, such as] constitutional make-up, perhaps genetic in origin, which predisposes individuals to both of them" [@daveysmithSmokingLungCancer2009]
  - Cornfield and colleagues demonstrated that such a confounder would have to be implausibly large to produce the observed association, and so was implausible
- (This matters most if you are using your analysis to make a causal claim)

## A sort-of criminological example

- As per @dingFirstCourseCausal2023 in general I haven't come across that many criminological papers which discuss unmeasured confounding (if you have done, please let me know!)
- One informal approach to unmeasured confounding is just to claim that the measures you _do_ have are sufficient...

## Talking about residual confounding

- @sampsonDoesMarriageReduce2006 wanted to assess the causal effect of marriage on crime - does getting married reduce a person's propensity to commit crime?
- They carry out some sophisticated statistical modelling, and conclude that yes marriage does reduce offending
- In caveating their analysis they say that:

> Probably the biggest limitation of our study is that the ... modeling approach we adopted assumes no _unmeasured covariates linked to both treatment and outcome_ [emphasis added]. In practice, the criterion of having no unobserved confounding is impossible to verify... however, we tried to counteract this limitation by exploiting ... [rich] data ... It is hard to imagine what the missing time-stable or time-invariant covariates are that would overcome the magnitude and robustness of results.

## Talking about residual confounding

> We thus argue that omitted confounders would have to be implausibly large to overturn the basic results

And they continue in a footnote that formally assessing the possibility of bias from omitted confounders was "beyond the scope of the current paper" and would require assumptions about the magnitude, direction, and functional form of potential biases that ultimately raise more questions than they answer" [@@sampsonDoesMarriageReduce2006, p499]

## Is this a good justification?

- There is disagreement about whether this justification is sufficient:
- "However, increasing the number of covariates is hardly a persuasive approach to ruling out potentially important confounders, as it is unlikely that one can adequately measure all such confounders" [@skardhamarDoesMarriageReduce2015]
- It's true that you have to make assumptions about the unmeasured confounding to know the impact on your results, and so it's necessarily speculative
- It may be possible to identify any such confounders in your particular study though


## Methods for measurement error, selection and confounding

- In general there aren't 'solutions' to these problems
- But the field of [quantitative bias analysis](https://link.springer.com/book/10.1007/978-3-030-82673-4) offers ways to describe the possible impacts of factors such as measurement error, selection effects and confounding on the results that you do have
- For example, these kinds of methods can show how large an effect of an unmeasured confounder would have to be to turn a statistically significant result to a non-significant one [(see e.g. the `tipr` `R` package for an implementation)](https://github.com/r-causal/tipr)


## So I should do a quantitative bias analysis?

- After all this discussion... maybe not!
- Epidemiologists who work in this area suggest you only need to conduct a full quantitative bias analysis if you 'claim to offer near-definitive conclusions' based on your study [@greenlandSensitivityAnalysisBias2014]
- I think the key thing is how we talk about our models and results - it is *us* that moves between the stories in the 'small world' of our model to the 'large world'
- We should have humility when describing our results, and draw readers' attention to the possible limitations of factors like those we have described
- For any particular problem technical fixes maybe be possible, the specifics will depend on the problem you are interested in, the data you have and the provenance of that data [@knoxAdministrativeRecordsMask2020a]

# Recap {background-color="#006938"}

## Recap

- If you are working with observational data (as is common in criminology), coefficients from a GLM may well be biased
- Quantitative bias analysis describes three secnarios where we may want to be critical about our results:
    - Measurement error
    - Selection effects
    - Unmeasured confounding
- The potential impact of these three factors depends on your specific data and analysis - there are no general solutions :shrug:

# Practical {background-color="#006938"}

- Using either the generative story that you came up with in Session One, or the description of the SOI from the first practical, describe the possible effects of:

    -   Measurement error?
    -   Selection effects?
    -   Unmeasured confounding?

Are you concerned about all these? Some more than others? Are there obvious steps you could take to address them?

# Break {background-color="#666666"}

---


# Turning coefficients into meaningful quantities of interest {background-color="#006938"}


---

## Making coefficients useful

- In the last session discussed reasons to be skeptical about our model coefficients, and looked at some methods which adjust coefficients to account for possible biases in the data
- In this session (and a bit more optimistically!) we are going to focus on how we can turn model coefficients into more meaningful and informative quantities
- This is a great way to make results more informative and accessible [@kingMakingMostStatistical2000]



## Regression coefficients: a referesher

- In simple (linear) models you read off a coefficient directly as the quantities that we are interested in
- In more complex models, such as generalized linear models, we often want to convert the coefficients to express results in a more interpretable way
- In poisson regression the model coefficients are also commonly expressed as rate ratios, by exponentiating the coefficients (we discussed this in Session Two)


## Translating coefficients: challenges

- However, approaches don't scale well with more complicated models, or complicated transformations [@mcelreathStatisticalRethinkingBayesian2020; @gelmanAveragePredictiveComparisons2007]
- A common way to translate model coefficients into more meaningful quantities is to use all the coefficients in the model to calculate predicted values of the outcome
- These are often called ['marginal effects'](https://marginaleffects.com/) (although this terminology is confusing)
- The terminology is confusing in part because there are [lots of related but subtly different quantities people call marginal effects](https://marginaleffects.com/vignettes/get_started.html)
- As a result, I like Gelman and Pardoe's terminology of 'adjusted predictive comparisons'



## Calculating a predicted values from models

- In the second practical we fit a GLM describing the relationship between violent crime and deprivation which gave us the results
  - Intercept = `2.4573632`
  - simd 2020 rank = `-0.0003029`

- Using our GLM formula, we know that the $log()$ of the expected violent crime count for a data zone with an SIMD rank of `0` is `2.4573632`
- (NB no such datazone will exist by definition, but that's okay!)
- We can calculate the expected violent crime incidents for this datazone with SIMD rank `0` by $exp()$-ing the intercept, giving us `11.674` violent crimes
- To an expected violent crime in the most deprived datazone in Scotland, we just add `1 * -0.0003029` (the coefficient for SIMD) to `2.4573632`, and then $exp()$ = `11.670`
- To calculate violent crime in the least deprived datazone in Scotland, we add `6976 * -0.0003029` (the coefficient for SIMD) to `2.4573632`, and then exponentiate =  `1.41`

## Calculating predictions

:::: {.columns}

::: {.column width="66%"}


```{r preds_again}
library(ggplot2)

data.frame(
  simd = seq(1, 6976)
) |> 
  mutate(pred_vio = exp(2.4573632 + simd * -0.0003029)) |> 
  ggplot(aes(x = simd, y = pred_vio)) +
  geom_line()



```


:::

::: {.column width="34%"}
In fact, we can calculate these predictions across the whole range of SIMD.

This gives a much more useful summary of the results compared to just the model coefficients.

:::

::::



## Uncertainty
- A key challenge is propagating the appropriate uncertainty in our results - especially in more complicated models when predictions might be a function of multiple parameters
- We can use simulation to propagate this uncertainty - and we can apply this approach to any generalized linear model and any Derived Quantity of Interest (DQI) - be it a marginal effect or something else
- The simulation process is described by @kingMakingMostStatistical2000 and implemented in the R package [clarify](https://github.com/iqss/clarify)
- This approach propagates both the uncertainty in the model's coefficients, and the correlations between the coefficients (for technical reasons that are beyond me, this is important [@gayleUsingQuasivarianceCommunicate2007])



# But first {background-color="#006938"}


---

## Simulating random variables: a brief introduction

:::: {.columns}

::: {.column width="66%"}

```{r rnorm_sim_v1}
library(tibble)
library(ggplot2)

var1 <- 
rnorm(
  n = 10000,
  mean = 0,
  sd = 1
)

tibble(
  var1 = var1,
) |> 
  ggplot(aes(x = var1)) +
  geom_histogram()

```

:::

::: {.column width="34%"}
`rnorm()` lets you simulate normally-distributed random variables.

Here we simulate a normally distributed random variable and plot its distribution
:::

::::


## Simulating random variables: a brief introduction

:::: {.columns}

::: {.column width="66%"}

```{r rnorm_sim_coef}
library(tibble)
library(ggplot2)

coef_draws <- 
data.frame(
  coef_sim  = rnorm(
  n = 10000,
  mean = -0.000303, # model coefficient
  sd = 0.0000106 # coefficient standard error
  )
)

coef_draws |> 
  reframe(coef = quantile(coef_sim,
                     c(0.025, 0.975)), # these are 95% confidence intervals
            interval = c("lower", "upper")) |> 
  mutate(exp_coef = exp(coef))

```

:::

::: {.column width="34%"}
If we plug in the coefficient for SIMD and the standard error of that coefficient, we can calculate confidence intervals for the coefficient.

The confidence intervals we calculated in the practical were $(0.9997178, 0.9996763)$

Our simulated intervals are the same to six decimal places.

:::

::::



---

## Back to our example

:::: {.columns}

::: {.column width="66%"}

``` {r clarify}

simd <- readRDS(url("https://github.com/benmatthewsed/statistical-methods-criminology-slides/raw/master/resources/simd_crime_sim.rds"))


mod_vio_simd_qp <- glm(vio_integer_sim ~ simd2020_rank, 
                      family = "quasipoisson",
                      data = simd)


```


:::

::: {.column width="34%"}
If we fit the model from the second practical again...
:::

::::

## Calculating predictions

:::: {.columns}

::: {.column width="66%"}


```{r preds}
library(ggplot2)

data.frame(
  simd = seq(1, 6976)
) |> 
  mutate(pred_vio = exp(2.4573632 + simd * -0.0003029)) |> 
  ggplot(aes(x = simd, y = pred_vio)) +
  geom_line()



```


:::

::: {.column width="34%"}
When we visualized the results from this model before we only got a point estimate for the expected number of violent crimes at each level of SIMD.

This doesn't express the uncertainty in these results as described by the model's standard errors.

:::

::::


## Back to our example


:::: {.columns}

::: {.column width="66%"}


``` {r clarify2}

library(clarify)

sims <- sim(mod_vio_simd_qp, n = 1000) 

sim_res <- sim_setx(sims, x = list(simd2020_rank = seq(1:6976)))

plot(sim_res)


```

:::

::: {.column width="34%"}
But by using the `clarify` package we can get confidence intervals around our predictions.

As it turns out in this case the confidence bands are quite small, and don't change our interpretation much - but we would not know this without calculating them.

The benefit of this simulation approach can also be used when we want to look at the effects of multiple variables, and where we want to describe more complex transformations of coefficients...
:::

::::


---

## An advanced example: victimization divides

- @hunterEquityJusticeCrime2016 wanted to describe how victimization inequality had changed over time
- They used the results of a fitted regression model to calculate a measure they call 'Victimization Divide'
- This measure is defined as:

$$
\frac{\left(\text{ratio of victimization rates in year 2} - 1\right) - \left(\text{ratio of victimization rates in year 2} - 1\right)}{\text{ratio of victimization rates in year 1} - 1}
$$
- This is analogous to exploring the percentage change in victimization inequality between two comparison years


## Victimization divides

- To calculate the ratio of victimization rates in years 1 and 2, @hunterEquityJusticeCrime2016 fit a GLM to predict the number of burglary victimization incidents experienced by households in England and Wales in 1993 compared to 2008/9
- They use the coefficients from these models as inputs into the Victimization Divide formula
- This is a nice example of translating model coefficients into a more more theoretically interesting quantity


## Victimization divides

- @hunterEquityJusticeCrime2016 conclude that burglary victimization inequality had increased for:
  -   single adult households compared to other households
  -   social renters compared to owner occupiers
  -   households without a car compared to those with one car
  -   households leaving their home unoccupied any amount of time on a typical weekday compared to those never leaving the home
  -   households in areas without neighbourhood watch compared to those with the scheme
  -   households earning at least £50,000 per annum compared to those on a £10,000–£29,999 annual income
  -   inner city residents compared to households in rural areas

## Victimization divides

- But @hunterEquityJusticeCrime2016's analysis used data from the [Crime Survey for England and Wales](https://www.crimesurvey.co.uk/en/index.html)
- So, in line with the story of these data, we should assess the uncertainty in these estimates which come from projecting from sample to population
- Just calculating victimization divides based on the point estimates of the model's coefficients doesn't account for this uncertainty
- We can do this with the @kingMakingMostStatistical2000 simulation approach

# But first...

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}


```{r mvtnorm_sim}
cor_data <- MASS::mvrnorm(
  n = 10000,
  mu = c(0, 0), # mu instead of mean
  Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2, ncol = 2) # Sigma instead of sd
) |> 
  as.data.frame()

cor_data |> 
  tidyr::pivot_longer(cols = c(V1, V2),
                      names_to = "variable",
               values_to = "value") |> 
  ggplot(aes(x = value)) +
  facet_wrap(~ variable) +
  geom_histogram()

```


:::

::: {.column width="34%"}
As we said before, we need to account for the correlation between model coefficients when simulating parameters from regression models.

If we use `mvrnorm()` the resulting simulations can be correlated. Here I set a correlation of 0.9.

`V1` and `V2` are both normally distributed...
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}

```{r cor_graph_2}
cor_data |> 
  ggplot(aes(x = V1, y = V2)) +
  geom_point() +
  geom_density_2d()

```


:::

::: {.column width="34%"}
... but highly correlated. 
:::

::::


## Simulating Victimization Divides

- To illustrate the process of simulating victimization divides we will work through this by hand
- In the exercises (and I would recommend in your own practice) we will use an `R` package written for this purpose
- First we fit the GLM
- Then we simulate a set of coefficient values from the regression model's variance-covariance matrix (usually at least 1000 draws)
- We calculate the VD for each one these simulated coefficient draws
- This gives us a distribution of VDs


---

## Simulating DQIs: worked example

:::: {.columns}

::: {.column width="66%"}
```{r vid_fn, fig.show = 'hide'}
#| echo: true

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```
:::

::: {.column width="34%"}
This is what the VD formula looks like in `R`
:::

::::

---

## Simulating DQIs: Getting ready

:::: {.columns}

::: {.column width="66%"}

```{r vd_prep}
#| echo: true

# load packages

library(MASS)
library(tidyverse)

# reading in data

dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )

# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```
:::

::: {.column width="34%"}
These are data from the Crime Survey for England and Wales showing victimization for men and women in 2015 and 2020. You can find more info on the data and approach [here](https://osf.io/k8j9e/)
:::

::::

---

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

```{r vd_models}
#| echo: true

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))
```

:::

::: {.column width="34%"}
We can fit a [$Binomial$ GLM](https://r.qcbs.ca/workshop06/book-en/binomial-glm.html) to calculate the log-odds of being a victim for men and women. This is like fitting a logistic regression to data we have already aggregated[^9]

In this example I fit a separate model for 2015 and 2020.

[^9]: Except in how the [residual degrees of freedom are calculated](https://stats.stackexchange.com/a/518934)

:::

::::

--- 

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

Model1 :
```{r vd_ests_2015}
#| echo: true
results_2015 <- 
broom::tidy(model2015) |> 
  mutate(est = exp(estimate))

results_2015
```

Model 2:
```{r vd_ests_2020}
#| echo: true

results_2020 <- 
broom::tidy(model2020) |> 
  mutate(est = exp(estimate))

results_2020

```


:::

::: {.column width="34%"}
With a binomial model we can exponentiate the model coefficients to calculate the [Odds Ratio](https://en.wikipedia.org/wiki/Odds_ratio) of being a victim for men and women.

Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having __11%__ greater odds of being a victim of crime. In contrast, Model 2 finds that men had a __5%__ greater odds of being a victim of crime than women in 2020 - however this difference does not meet the 95% threshold for statistical significance.
:::

::::

---

## How much has victimization inequality changed?


:::: {.columns}

::: {.column width="66%"}

```{r vd_calc}
#| echo: true
victim_divide(
  base_y1 = results_2015$est[[2]],
  base_y2 = results_2020$est[[2]]
  )
```


:::

::: {.column width="34%"}
Calculating the VD for these two results, based just on the odds ratios from the two models, shows that victimization inequality decreased by _52%_ between the two years. 

From the models' point estimates, we would conclude that victimization inequality between men and women fell by more than half between 2015 and 2020!

But this doesn't factor in the uncertainty in the models' coefficients.
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim}
#| echo: true
# set seed
set.seed(nchar("vict divide") ^ 4)

n_sims <- 1e4

draws_2020 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2020),
    Sigma = vcov(model2020)
  )

draws_2015 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2015),
    Sigma = vcov(model2015)
  )
```


:::

::: {.column width="34%"}
We can pass the models' coefficients and variance-covariance matrices to `mvrnorm` from the `{MASS}` library. 

This gives us a set of 10,000 coefficients for each model which are consistent with the uncertainty in the models' results.
:::

::::


---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_calc}
#| echo: true
draws_2015 <-
  draws_2015 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

draws_2020 <-
  draws_2020 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

# combine the results
sim_dat <-
  tibble(
    vd = victim_divide(base_y1 = draws_2015$est,
                       base_y2 = draws_2020$est)
  )


```


:::

::: {.column width="34%"}
Once we tidy the results up a bit, we can pass these simulated draws to our `victim_divide()` function to get a distribution of VDs.

From these we can calculate confidence intervals to see how sure we can be that victimization inequality between men and women really did fall by 52%.
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_res}
#| echo: true
sim_dat |> 
  reframe(
    vds = quantile(vd, c(0.025, 0.5, 0.975)),
    centile = c("2.5%", "50%", "97.5%")
    )


sim_dat |> 
  ggplot(aes(x = vd)) +
  geom_histogram(binwidth = 0.1) +
  scale_x_continuous(limits = c(-2, 2)) +
  geom_vline(aes(xintercept = 0))

```


:::

::: {.column width="34%"}
We can see the 2.5 and 97.5 percentiles of the simulated VDs, as well as the histogram of their distribution. 

We can use these intervals to approximate a statistical significance test - if the interval includes zero the coefficient is not 'statistically significant' at the standard level.

These intervals include zero, so the apparent 52% reduction in victimization inequality would not be 'statistically significant' at the standard level.
:::

::::

---

## Simulation is great!

- The beauty of this simulation approach is that it generalizes to _any DQI_ that is a function of the model's parameters, such as VD, and to _any GLM_ [@kingMakingMostStatistical2000]
- This can help us match how we present our results to how our data was generated (in this case, by a sample survey)
- Say that instead of the VD were interested in the absolute difference in predicted victimization rates after controlling for other factors (like a marginal effect)
- Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating the model's uncertainty into our estimates
  - This would involve the extra step of simulating from the $Poisson$ distribution to map the simulated coefficients back into counts (a step we don't cover here)


---

## Simulation in practice

- In the second example we did the simulation by hand. This is useful as a way to understand the method, but whilst it's good to know how this works, in practice there are R packages which can do this for us. 
- One good option is [clarify](https://cran.r-project.org/web/packages/clarify/index.html), as used in the first example. It's likely that professionally developed and maintained software will be less error-prone and more efficient than writing your own!
- You also need to run enough simulation draws to get a good estimate of the uncertainty. The default for clarify is 1000 draws.
- Simulations can be very time consuming! Especially if your model is complicated and you are making predictions for a lot of cases
  - More simulations are always better, but will take longer - so there is a pragmatic aspect to how much time and computer resource you have to run simulations.


---

## Practical limitations and alternatives
- There is inherent uncertainty in simulation results, so it's crucial to set the seed for the random number generator so you get the same results every time. 
- There are also some subtleties in how you use the simulated results [@raineyCarefulConsiderationCLARIFY2024]
- There are also other ways to do describe this uncertainty:
  - The ['delta method'](https://marginaleffects.com/vignettes/uncertainty.html#delta-method) uses calculation, not simulation, but relies on a [Normal approximation which may not hold](https://iqss.github.io/clarify/#introduction). This is the default method used by the [marginaleffects](https://marginaleffects.com/) `R` package

---

## Conceptual limitations

- More broadly, we need to remember that simulation expresses the uncertainty captured _by our model parameters_
- But remember that our model may be wrong [@greenlandIntervalEstimationSimulation2004] - we spent all of Session Three discussing this!
- Our model still makes a load of assumptions - basically that we've fit the right model and that we had the right data - that are baked into our simulations
- It is possible to incorporate both bias analysis and simulation into the same analysis [@greenlandIntervalEstimationSimulation2004, @foxSASCodeProbabilistic2023], although this is far from common practice


## Recap

- Model coefficients can be difficult to interpret
- Combining coefficients to make predictions of the outcome is a powerful way to express quantitative results more meaningfully
- Simulation, such as using the `clarify` package, offers a powerful and flexible way to describe the uncertainty in these predictions that can be applied to any GLM
- ... but our simulations are only as good as the underlying model (and so open to all the criticisms we covered in Session Three)

## Practical


# Break {background-color="#666666"}

---

# Ethics and commuicating statistical results  {background-color="#006938"}

## Ethics and model coefficients

- In the past two sessions we've been focusing on telling the story of our analysis by critiquing model coefficients and then translating them into more meaningful quantities
- In this session we will touch on some ethical or normative issues when presenting quantitative results, specifically looking at:
  - Essentializing and stereotyping
  - Ethics and language use describing quantitative results
- This is not a general primer on ethics when using statistical methods for social research: we don't cover anonymity, confidentiality, no harm to participants and so on
- There also a particular set of ethical issues around workflow in quantiative social science - or if you like, how we tell the story _of_ our research - which are crucial in any quantitative analysis but that we won't cover [@gayleStarkRealitiesReproducible2022]


## Essentializing, stereotyping and the tyranny of the means
- The typical focus of regression results is group average effect, often focusing on whether an average difference between groups in the levels of our outcome is statistically significant [@merloTyrannyAveragesIndiscriminate2017]
- Focusing only on group averages can imply that all members of the group are the same [@curtisQuantitativeCriticalismGuidelines2022]
- And, even if we have a statistically significant difference between two groups (say, people who live in high deprivation areas versus other areas) in the count of offences convicted in court, on its own this doesn't tell us much about how likely any individual member of that group is to be convicted



## Tyranny of the means in GLM

- This problem is particularly acute for GLMs, because the 'expected value' of the outcome (as we were calculating earlier) does not capture the variation in the outcome expressed by the GLM family
- One way to assess this is to plug the expected value calculated through the GLM through the probability distribution we have specified
- In our $Poisson$ models before, we can use simulation again to describe the distribution of counts associated with a given ($\lambda$)

## Tyranny of the means in GLMs

:::: {.columns}

::: {.column width="66%"}


```{r outcome_sims}


library(ggplot2)

set.seed(nchar("poisson sims"))

data.frame(
  sims = rpois( # draw from a Poisson distribution
    n = 10000, # 10,000 simulations
    lambda = (1 * -0.0003029 + 2.4573632)) # the expected number of crimes for the most deprived neighborhood
)|> 
  ggplot(aes(x = sims)) +
  geom_histogram(binwidth = 1) +
  scale_x_continuous(breaks = seq(0, 10)) +
  geom_vline(xintercept = (1 * -0.0003029 + 2.4573632))


```

:::

::: {.column width="34%"}
This code takes the expected value of crimes for the most deprived neighbourhood based on our simple model of SIMD and police recorded crime, and then uses the `rpois` function to simulate from the $Poisson$ distribution at this expected value.

The results show that around 8% of the time we would expect that a neighbourhood with this expected value of crime would actually experience no recorded crimes at all.

:::

::::

## The tyranny of the means continued

- This issue of essentializing is  particularly acute if we want to use a 'risk factor' derived from a quantitative analysis as a diagnostic or predictive tool [@merloTyrannyAveragesIndiscriminate2017]...
- ... and particularly when we are modelling binary outcomes (so events that either happen or do not)
- The reasons for this are quite technical, but are well described in @merloTyrannyAveragesIndiscriminate2017
- When - as is often in criminology - analysis is focusing on comparing demographic groups and their exposure to crime or the criminal justice system, a focus on average differences can unwittingly perpetuate negative stereotypes, as well as only providing a limited picture of the empirical results



## Framing: the importance of language

- More conceptually, it's important to think about how we frame the results of any analysis.
- We will consider this in the broader societal context of data collection collection, rather than in the context of any particular dataset
- The key point is that data never speak for themselves [@dignazioDataFeminism2020]



## Example one: Ethical issues in describing charts

- @dignazioDataFeminism2020 re-represent some descriptive statistics from a paper by @kabaDisparitiesMentalHealth2015 to illustrate the normative choices faced when visually presenting results
- The @kabaDisparitiesMentalHealth2015 is a study of (among other things) rates of mental health diagnosis in prisons in New York, broken down by ethnicity
- @kabaDisparitiesMentalHealth2015 suggest that people from Black or Hispanic backgrounds may be more likely to be seen as criminal rather than experiencing mental ill-health, and so may be less likely to be given a mental health diagnosis
- @dignazioDataFeminism2020 illustrate ways in which one of the findings from this study could be framed:

## Example one: Ethical issues in describing charts

![](resources/data-feminism-fig-1.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Example one: Ethical issues in describing charts


"The study that produced these numbers contains convincing evidence that we should distrust diagnosis numbers due to racial and ethnic discrimination. The first chart does not simply fail to communicate that but also actively undermines that main finding of the research." @dignazioDataFeminism2020


## Example one: Ethical issues in describing charts


![](resources/data-feminism-fig-2.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Example one: Ethical issues in describing charts

- "By naming racism and then talking about people of color in the title, the graphic reinforces the idea that race is an issue for people of color only."


## Example one: Ethical issues in describing charts


![](resources/data-feminism-fig-3.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Framing

@dignazioDataFeminism2020 propose this third framing to focus on the advantages of White group (rrather than the disadvantages of the Black and Hispanic groups), and contend that the subtile (emphasising this advantage) better fits the researchers' conclusions



## What are our ethical responsibilities?

- @dignazioDataFeminism2020 argue that "Placing numbers in context and naming racism or sexism when it is present in those numbers should be a requirement... for data communication"
- [Data Feminism](https://data-feminism.mitpress.mit.edu/) is mostly aimed towards data scientists and data journalists, rather than academics. Does this change how we should view their recommendations?
- The value of this exercise is not to say that any of these framings is 'right' (although for the reasons @dignazioDataFeminism2020 outline we prefer some to others), but that they reflect different theoretical positions, and give different emphases to contextual factors - factors outside our datasets



# Practical

## Practical

- Revisit the [three graphs from @dignazioDataFeminism2020](https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)
- Which of these would you use to present results to:
  - An academic conference
  - A government report
  - An infographic aimed at the general public

# Overview  {background-color="#006938"}

## Overview

- Phew! We've covered a lot of ground today
- The key idea of this workshop is that the story of how our data came to be ('data provenance') should influence our analysis and the story we tell about our results
- We looked at common data provenance stories of police recorded crime and victimization surveys
- We talked about GLMs as a common way of telling stories about the small world of our data
- We looked at how we might want to critique the stories our GLMs are telling, using tools from quantitative bias analysis
- And how to use simulation approaches to tell more meaningful stories from our results
- We discussed some ethical issues that come up when we present our results

## Where next?

- Data provenance: Over to you!
- GLMs (and beyond)
  - (Statistical Rethinking)[https://github.com/rmcelreath/stat_rethinking_2024]
  - (Quantitative Social Science Methods, I (Gov2001 at Harvard), Gary King)[https://www.youtube.com/watch?v=qs2uCuDL2OQ&list=PL0n492lUg2sgSevEQ3bLilGbFph4l92gH]

- Quantitative Bias Analysis: :shrug: Maybe (_Quantitative Assessment of Systematic Bias: A Guide for Researchers_)[https://journals.sagepub.com/doi/10.1177/00220345231193314] or [@kawabataQuantitativeBiasAnalysis2023]?
- Simulating meaningful quantities from results: (`{clarify}`)[https://iqss.github.io/clarify/] and (`{marginaleffects}`)[https://marginaleffects.com/]
- Ethics: (Data Feminism)[https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3]


## Course Feedback

- Please take a few minutes to fill in the course evaluation form: https://www.ncrm.ac.uk/surveys/edin


# Thank you!

## Bonus slides


## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Reconviction rates could be used to rank performance across different local
authorities. However, there is an inherent problem in using this approach since
it implicitly assumes that a difference in reconviction rates reflects a ‘real’
difference between local authorities. In reality, all systems within which these
local authorities operate, no matter how stable, will produce variable outcomes
in the normal run of events. In particular, outcomes in local authorities with
smaller sized populations tend to vary more than those in local authorities with
larger populations. The question we need to answer is therefore: Is the
observed variation more or less than we would normally expect? "

:::

::::



## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"The funnel plot is a simple
statistical method that takes into account the variability of different sized
populations and so highlights whether there are differences that may be
attributed to some other special cause... The plot takes into account the increased variability of the local
authority groups with smaller populations, where a small increase in the
number of reconvictions may lead to a large percentage change in the
reconviction rate.

:::

::::




## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Rates for local authority groups which lie inside the funnel are not significantly different from the national rate, and we can then usefully
focus on possible explanations for rates which deviate significantly from the
national figure.

Whilst this
is useful for highlighting that there are practical differences in reconviction
rates between each local authority group, even after taking into account
differences in population sizes, it does not allow us to identify if this disparity is
due to variation in the characteristics of offenders in each area or a variation in
practices between different local authority groups. Different offender
characteristics between local authority groups could include: age, sex, crime,
disposal, deprivation, etc.

:::

::::


## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-02.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Chart 12 is standardised to take into account some of the differences between
local authority groups attributable to the characteristics of offenders, such as
the number of previous offences, sentence, sex, and age. It provides the
standardised reconviction rates3 against the observed number of offenders
minus expected number of offenders. Since all local authorities groups are
within the funnel it suggests that the apparent differences in reconviction rates in Chart 11 are primarily attributable to either the variation in the
characteristics of the offenders, the type of crime they committed, or the
sentence they received, rather than differences in ‘performance’ between the
local authority groups."

:::

::::

## Reflections on this example

- Even though the reconviction cohort could be thought of as 'full population' data, Scottish Government use ideas from inferential statistics to interpret the data
- I think this well justified
- There are two implied counterfactuals
  - What could things have looked like for this cohort given the "variable outcomes" we would expect "in the normal run of events"?
  - What would reconviction rates have looked like if all local authorities had the same distribution of characteristics of people who offended, convicted for the same offences and receiving the same sentences


## (Bonus) Example Three: Ethnic disparities in Covid Fines

- https://blogs.ed.ac.uk/edinburghlawschool/wp-content/uploads/sites/8261/2023/03/NPCC-Report-March-2023-final-1.pdf
- Wanted to calculate disparities in Covid fines issued across England and Wales to people from different ethnic backgrounds
- Covid fines data were from 2020-2021, but the most recent data on ethnicity by Police Force Area was from 2016
- "This may mean that the FPN rates by
ethnicity – and so our calculated disparity rates – are inaccurate due to changes in the population denominator used to calculate the rates. This could particularly affect
PFAs with small ethnic minority populations, where small changes in the estimated
ethnic minority population could lead to large differences in the estimated FPN rates."



## Same observables, different outcomes

![](images/qlp-results-figure.png){fig-alt="Estimated Adult Conviction Trajectory Probabilities by Sex, Indigenous Status and Childhood System Contact."}

## How we described the figure

"Our results show substantial variation in adult conviction trajectories within sex and Indigenous status groups for people with identical observable characteristics, as well as between our four demographic groups... Even though we see a strong association between Indigenous status, sex and membership of the High/Persistent conviction trajectory (see Appendix 4), it would be wrong to assume that all Indigenous men with multiple childhood system contact end up with High/Persistent adult conviction trajectories—our model estimates that almost half of Indigenous men would _not_ end up with a high-rate, persistent adult conviction trajectory even with this substantial childhood adversity."

https://link.springer.com/article/10.1007/s40865-022-00204-z



## References