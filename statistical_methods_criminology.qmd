---
title: "Statistical Methods for Criminology"
title-slide-attributes:
    data-background-color: "#40666e"
author: "Ben Matthews"
bibliography: references.bib
execute: 
  echo: true
format:
  revealjs:
    theme: [clean.scss]
    smaller: true
    scrollable: true
---



## Welcome!

- Who we are
- [workshop outline](https://github.com/benmatthewsed/ui-data-visualization-course/blob/master/course_outline.md)
- [Code of conduct](https://github.com/benmatthewsed/ui-data-visualization-course/blob/master/code_of_conduct.md)


---

## Learning outcomes

By the end of the workshop you will:

- Understand the main forms of criminological data 
- Understand how data about crime and victimization can be understood using the General Linear Model (GLM)
- Be aware of issues that can arise in interpreting GLM coefficients due to measurement error, selection bias and omitted variables
- Be aware of ways to transform GLM coefficients into meaningful Derived Quantities of Interest (DQI)
- Understand ethical issues around how results from statistical methods fit to criminological data are discussed


---

## Course outline

_add this here_


---

## Points of order

- __Ask questions whenever__ :thumbsup: You can do this through the Teams chat facility __{demo this now}__
- We've structured the sessions with regular breaks, but __if you need to leave just leave__!
- Materials will live online ([slides](https://benmatthewsed.github.io/statistical-methods-criminology-slides/statistical_methods_criminology.html#/title-slide),) so you can access them any time


---

## `R` set-up

- You can follow along with the materials in a local installation of `R` and `RStudio` on your own computer


---

## Who you are

- Your __research interests__
- __Why__ this course?
- What do you want to __achieve__?


# Before we begin... {background-color="#40666e"}

---

## Statistical analysis and stories

- This workshop is _also_ about stories. Specifically:
  - The story of how the data in your spreadsheet came to exist
  - The story you tell about these data based on statistical analysis
- I want to convince you that the first of these two stories should filter through into every decision you make during analysis, and so should determine the second
- You need to know how your data came about to be able to analyse it properly

---



# Session One: Types of criminological data

---

## Types of criminological data

- Criminology as a 'rendezvous discipline' as David Downes once said
- Criminological data could be
  - Administrative data from the justice system (police, courts, prisons, probation... etc)
  - Secondary survey data (e.g. victimization, offending, fear of crime... etc)
  - Newspaper reports (e.g. collations of stories on police use of force or homicides)
  - Social media data (e.g. fear of crime)
  - 'Digital trace' data from darknet drugs transactions
  - ...

---

## Key types of criminological data

- Here we focus on police recorded crime and victimization surveys as common forms of criminological data
- If you are interested in another form of data, please do ask!

---

## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
- How has the data been collected and/or by whom or by what?
- What/who is included and what/who is excluded?
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
- What are the relevant definitions and concepts that govern the data/data collection?"

[@8347e95e012d4212a5ee429a18ee592e, p228-229]

---

## How does an event become a crime statistic?

![Recorded crime](resources/recorded_crime_flowchart.pdf){style="transform: rotate(90deg);"}

## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of police reporting activity on crime levels
- How has the data been collected and/or by whom or by what?
    - https://www.gov.scot/publications/recorded-crime-scotland-2023-24/pages/17/
- What/who is included and what/who is excluded?
    - Excludes: crimes not reported and for which there was insufficient evidence
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Depends? National statistics are routine collections, there are other data though https://www.law.ed.ac.uk/sites/default/files/2022-08/FPN%204th%20report%20-%20FINAL.pdf
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - Recorded crime data bulletin - yes, cleaned and standardized
- What are the relevant definitions and concepts that govern the data/data collection?"
  - The current [Scottish Crime Recording Standard](https://www.gov.scot/publications/scottish-crime-recording-standard-crime-recording-counting-rules-2/) is 550 pages long (!). I have not read it (and don't intend to!)

---

## Implications of this story

- Reporting on crime levels - incentives to reduce figures?
    - https://osr.statisticsauthority.gov.uk/publication/the-quality-of-police-recorded-crime-statistics-for-england-and-wales/
- Excludes incidents not reported (see dark figure of crime)
- Figures represent (unknown?) mix of behavioural and system effects
  - Urban areas have high police recorded crime rates because ...
      - that's where crime is?
      - that's where poverty and disadvantage are?
      - that's where disproportionate surveillance and punishment are?

---   

## Implications for analysis: Population or super-population?

- One reason people use inferential statistical models is to generalize from the data they have to a wider population. With recorded crime data, is this the whole population?
-  outline two possible approaches to conceptualizing police recorded crime data: a population or superpopulation approach [@verlaan2024use]
- Population approach: if you have police recorded crime data for Scotland in 2023 the data you have is all you could ever have. You don't need to fit statistical models to generalize from the data you observe to a wider population because *there is no wider population*.
- Superpopulation approach: you can think of Scotland in 2023 as 'drawn' from from the population of the UK, or Europe, or the whole planet. Alternatively, data from Scotland from 2023 may be seen as a sample from Scotland from 2023 and 2024, or 2023-2030 and so on. You might also want to construct a counterfactual of what crime would have been like in Scotland in 2023 if unemployment was 5% lower or 5% higher Gelman (2011)

---

## Dangers of inferential statistics with recorded crime data

- It wrongly leads people to assume that their results are generalizable [@verlaan2024use]
- It can lead people to undervalue actual observed differences in their data. For example, researchers may deny that an association between two variables exists in their dataset if it is not statistically significant

---

## Dangers of _not_ using inferential statistics with recorded crime data

- confidence intervals should be reported even when describing statistics from the full population, especially if the results are to be used to make predictions or inform policy. Importantly, even if _you_ are not be interested in prediction or policy-making, you can't control how others will use your results [@d.redelingsWhyConfidenceIntervals2012]
- Different sized populations (e.g. local authorities) have different levels of variability [@spiegelhalterFunnelPlotsComparing2005]. Crime rates in small places will be more volatile - and so have more uncertainty - year-on-year than those from large areas


---

## Survey data

- In response to known issues with measuring levels of crime with administrative data, since the 1980s criminologists in some parts of the world have been surveying the public to ask about their levels of victimization.
- Scottish Crime and Justice Survey​
- Crime Survey for England and Wales​
- Equivalents in other countries, primarily in Western Europe, North America and Australasia ​
- Smaller geographical scale surveys such as The Mayor's Office for Policing And Crime (MOPAC) Survey in London

---

## Survey data: benefits

- Typically ask people about their experiences of victimization in the last year​
- Can measure crime that isn’t reported to the police​
- Usually don’t ask about people’s offending behaviour​
- Good for measuring common crimes, bad for measuring rare crimes

---

## Answering our key questions

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of government reporting on experiences and attitudes of criminal justice
- How has the data been collected and/or by whom or by what?
    - Collected by professional survey agencies
- What/who is included and what/who is excluded?
    - Random sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)
  - For those who agree, one adult (age 16 or over) in each household is randomly selected for interview​
  
## Implications
  
- Therefore, *by design* no:​
  - Children​
  - Homeless people​
  - People living in communal establishments (e.g. students, people in prison)
  - Also in practice may exclude those in 'fragile, disjointed households' [@carr-hillMissingMillionsMeasuring2013]
  - For SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​

But this excludes people, students, people in prison, and those living in refuges. However, “Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”
  
---
  
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Routine reporting
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - When it gets to you it has usually been validated. Some information is also restricted, such as 'capping' counts of victimization - we'll talk about this later...
- What are the relevant definitions and concepts that govern the data/data collection?"
  - You can read the question wording online! But... the questions included can changes over time (https://www.sccjr.ac.uk/wp-content/uploads/2024/02/group-two_hackathon-output.pdf)
  
  
---
  
## Implications for analysis

- With surveys we almost always do want to do inferential statistics!

- Surveys are very rarely random samples; most come with weights
- Use the weights for descriptives to gross up to national populations
- Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model - it may depend on the specifics of your survey
- But, weights will only help adjust the data you see towards the target population in the survey design frame. Weighting cannot adjust for populations who are excluded from the survey by design.



--- 

# Recap:
# Data provenance determines what types of analysis are appropriate for a given aim
# Recorded crime and victimization surveys exclude some incidents/population groups by design
# There are pros and cons of using inferential statistics with recorded crime data



# Break {background-color="#666666"}

---


# Practical

- Describe SOI
- Give examples from my thesis of being inconsistent

- Do I need significance testing here?

- What's the story of *your* crime data?



---


# The general linear model


---

## Large worlds and small worlds

- Our spreadsheets and model coefficients can only summarize the `small world` for us, and omit some of the complexity of the `large world`
- In this session we'll overview the most common[^3] way in which criminologists understand the 'small world' of their spreadsheets: the general linear model (GLM)
- But be warned, as soon as we want to understand the large world we can run in to problems if all we focus on are the coefficients..

---

## A disclaimer

- If you are taking this course I assume that you have some familiarity with linear models of some description. This session is therefore mostly a refresher.

---


## A(nother) disclaimer

- You could be possible to spend months and months studying GLMs and their various extensions. Some specific flavours of model that we *don't* cover, but which may be useful for your own work include:
- Mixed/multilevel/hierarchical/etc (see e.g. https://www.cmm.bris.ac.uk/lemma/)
- Additive models (GAMs) (which are less common in criminology, but are very useful; https://noamross.github.io/gams-in-r-course/)
- 'bespoke' models (very infrequently used in criminology to my knowledge, https://betanalpha.github.io/assets/case_studies/generative_modeling.html, but see e.g. https://josepinasanchez.uk/wp-content/uploads/2018/09/bsc-presentation.pdf)

## Anatomy of a GLM

-   Stochastic (or random) component
-   Systematic component
-   Link function that converts between the parameter estimates and the form of the outcome (we'll say more about this later)

## Anatomy of a GLM

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## The linear model

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## Logistic regression

For logistic regression we have:

$$
\begin{align*}
y_i \sim & {Binomial} (n, p_i) \\
{logit(p_i)} & = \alpha + \beta (x_i),
\end{align*}
$$ 

for logistic regression, $n$ = 1, and we are just interested in modelling $p_i$, the probability of the outcome. The nice thing about this model formulation is that the ${logit}$ link function makes sure that all the probabilities the model estimates are between 0 and 1.


## Count data

Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims. 
(Whilst we may see crime data be re-expressed as *rates* per 1,000 population, before this they are counts.)

## Poisson model

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The coefficients from Poisson models (which range from $-\infty$ to $\infty$) are converted to be predicted counts that are non-negative integers via the $\log$ link function. This lets us have coefficients which can take any value (-2! 0.5!), but then convert these to counts as our count outcome demands.

## why not just use a linear model?

If you really want to you can just use a linear model for count data. This will still give you an accurate model of the mean of your outcome. However, there are two main problems with this approach.

## Negative counts?

First, if you have small counts (say, if you were modelling homicides in Scotland) the uncertainty in the mean estimate may give you a confidence interval below zero:

```{r}
library(dplyr)

set.seed(12346)

n_draws <- 1e3

dat <- 
data.frame(
  y = rpois(n = n_draws,
            lambda = 0.001) # draw from poisson distribution with mean 0.001
)

dat |> 
  count(y)

lm(y ~ 1, data = dat) |> # fit intercept-only model with normal outcome
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error)


glm(y ~ 1, # fit intercept only model with poisson outcome
    family = "poisson",
    data = dat) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low))

```

Here the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive). 


## Non-constant variance?

Second, the standard linear model assumes constant variance. But in practice we probably want more variance for larger counts. 

See figure at https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html

## Interpretting coefficients

- Per UCLA OARC: "for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant."

- The most straightforward way to interpret coefficients from Poisson models is to exponentiate the coefficient value. This converts the beta coefficient into an Incident Rate Ratio (https://stats.oarc.ucla.edu/stata/output/poisson-regression/)


---

## Problems with Poisson: over-dispersion

-   poisson assumes mean and variance are the same (in that there is only one term in the model, $\lambda$, which describes both the average and the variability around the average)

- This is quite a brittle assumption though, and there's no reason that real-life data has to obey it. So what? If your data are over-dispersed (the conditional variance exceeds the conditional mean), you will get standard errors that are too small.
- p-values are too optimistic
- need some way to account for over-dispersion

---

## Solutions?

### What do to? about over-dispersion

- If all you care about is your standard error being too small you can use quasi-poisson
- This gives same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it's also faster than the standard alternative...
- The negative binomial is a flavour of statistical model ^[okay, strictly speaking there are different varieties of negative binomial model. So... flavours? [@hilbeModelingCountData2014]] for count data which has an extra "dispersion" parameter, which allows you to model the over-dispersion directly
- People disagree as to which is preferable



- Example of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf

---

##

Also - rates!

## Practical

In this practical we'll fit some count models in R

# Morning recap {background-color="#40666e"}

## Morning recap

- The story of how our data came to be influences how we should analyse it
- GLMs are the most popular way to tell stories about the `small world` of our data


# Lunch :sandwich: {background-color="#666666"}

---



# Measurement error, selection and confounding


---

## From small world to large world?

- There are lots of reasons we may be skeptical about applying conclusions drawn from 'small world' of our data and model to the 'large world' we live in
- In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models
- Again, books and books have been written about each topic, so this will only offer a brief overview of each, with links to further reading for those who want to know more.

---

## Are coefficients sufficient?

- We should be skeptical about the numbers in our spreadsheets and the coefficients that summarise them
- The numbers may be (predictably) inaccurate (measurement error);
- they may omit some variables that we theoretically think are important (confounding), or;
- they omit some people/cases we care about (selection effects)
- These are problems that are hard to solve based on the data we observe alone
- There is a large literature in epidemiology study these issues called __Quantitative Bias Analysis__
- 

---



# Act One: Measurement error

---

## Measurement error

- Measurement error is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet
- GLM assumes no measurement error
- In practice we know criminological data are likely to be measured with some degree of error.
- Police recorded crime data is not a perfect measure of the amount of crime 'out there' in society. As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.


---

## Measurement error in practice: police recorded crime

-   From our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data.
-   So if we are interested in, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an *under* count.
-   The many years of work comparing crimes recorded to the police with victimization surveys - the 'dark figure of crime' - attests to this.

## Measurement error in recorded crime

- Measurement error in recorded crime is _proportional to the amount of crime_ (Pina Sanchez et al 2023)
- There is a larger gap between the amount of crime recorded and 'true' crime levels in areas with large numbers of recorded crimes compared to areas with very low recorded crime


- Measurement error in crime due to e.g. lack of willingness to report may itself be related to other variables in your analysis (e.g. economic inequality)
- This is called 'differential' measurement error

## `rcme`

Pina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.

> Work through their example here? https://osf.io/preprints/socarxiv/sbc8w

Open questions - what about survey data? Models other than linear models? What about measurement error in independent variables?

- At the moment seems like this focuses on measurement error with recorded crime as the dependent variable

---

## Example Two: Measurement error in victimizaton survey data

-   Historically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete. In 2019, ONS said that "Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way." (https://doc.ukdataservice.ac.uk/doc/7280/mrdoc/pdf/7280_csew_improving_victimisation_estimates_2019.pdf)

-   However, it also means that people who experienced more than five incidents of a particular 'series' crime type did not have their data accurately recorded

## Impact of capping on analysis

-   This was particularly important for women's reporting of violent victimization (Walby et al. 2015) - women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year. A second measurement error issue came from the '97 code' - the option to report the number of incidents experienced as '96/too many to count'. Instead, based on the domestic violence literature Walby and colleagues propose using an estimate of 60 incidents for those who report the 97 code.

- This would bias estimates of total crimes , but not prevalence (the number of victims)

-   Sometimes it's possible to use uncapped data

---

## Adventures in measurement error

- The two examples we've looked at have focused on measurement error in the outcome (recorded crime and victimization)
- But it also matters where in your model the measurement error manifests (/manifests more)
- If you have error in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results (meaning that they are less likely to be statistically significant).
- Measurement error in your key independent variable may bias your regression coefficients *downwards*, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. 
- But if there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to 'under controlling' - and finding statistically significant coefficients where there are none. 
- It's quite context dependent - so you need to know the likely source of error in your specific dataset
- Other than in simple scenarios *we may not know what impact it is having*. Uh oh!


## Residual or unmeasured confounding

Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we don't have a measure for this
- Famous example of smoking and lung cancer
- There are ways you can try to quantify this discussed in the field of quantitative bias analysis. I have not seen these methods used in criminology very much
- One informal approach to unmeasured confounding is to claim that your measures are good enough to make it not a problem

-   because we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).

*The tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven't measured*, although Peto suggests that these in practice are hard to distinguish

*need some criminological examples here*


Probably the biggest limitation of our study is that the IPTW modeling
approach we adopted assumes no unmeasured covariates linked to both
treatment and outcome. In practice, the criterion of having no unobserved
confounding is impossible to verify—the data in any observational study
provide no definitive information (Robins, Hernán, and Brumback, 2000).
As discussed above, however, we tried to counteract this limitation by
exploiting what we believe are rich individual baseline data and timevarying covariates over the full life course in order to model the
propensity to marriage. It is hard to imagine what the missing time-stable
or time-invariant covariates are that would overcome the magnitude and
robustness of results. From IQ to the cumulative history of both the
outcome and the treatment, we accounted for 20 baseline covariates and
approximately a dozen time-varying confounders measured from widely
varying sources—many of which predict the course of marriage as
theoretically expected (table 1). 



We thus argue that omitted confounders would have to be implausibly
large to overturn the basic results obtained under a number of different
model specifications and assumptions.22

22 A formal sensitivity analysis (see Robins, 1999: 167–73) is beyond the scope of the
current paper. Moreover, such analyses require assumptions about the magnitude,
direction, and functional form of potential biases that ultimately raise more
questions than they answer

(https://psycnet.apa.org/record/2008-07491-001)

- It's true that you have to make assumptions about the unmeasured confounding to know the impact on your results, and so it's necessarily speculative

- But this is possible!

"However, increasing the number of covariates is hardly
a persuasive approach to ruling out potentially important confounders,
as it is unlikely that one can adequately measure all such confounders"

- from their study they find:

```{r}

# from table 2 in Sampson, Laub and Wimer, p 491 
# https://scholar.harvard.edu/files/sampson/files/2006_criminology_laubwimer_1.pdf

library(EValue)

evalues.RR(est = 0.572, lo = 0.511, hi = 0.640)
  bias_plot(0.572, xmax = 10)

```

From this, the unmeasured confounder would have to be associated with an almost three-fold increase in the risk of offending, and must be almost three times more prevalent in those married than those not married, to explain the observed risk ratio.

The question then becomes... how plausible is this?



See https://cran.r-project.org/web/packages/EValue/vignettes/multiple-bias.html

 https://link.springer.com/book/10.1007/978-3-030-82673-4

"The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding"

-   so this is based on the idea tat we can identify an "implausibly strong" confounder, which is reasonable.

-   One approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.


## Selection effects

Selection bias arises when there are people who we would have liked to observe in our study but we don't observe them, and this lack of observation is related to their characteristics. Put another way, we can think of selection bias as affecting the rows of our dataset - there are some rows that are missing that we would like to see.

[@greenlandSensitivityAnalysisBias2014]


## An example of selection bias


Imagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.

Police collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?

The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”

In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset *would* know this.

Even if we *do* know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we'd need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)

We're going to hear a lot about 'strong and untestable assumptions'.

## Uh oh

“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”

\[INSERT FIGURE 2 FROM KNOX\]

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.

Others [@gaeblerCausalFrameworkObservational2022]suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).

## Solutions?

- @knoxAdministrativeRecordsMask2020a suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify *causal* relationships *there is no general solution* that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records.
- The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.

---

## Should I do a quantitative bias analysis?

- It depends
- ... so you only need to bother with this stuff if you 'claim to offer near-definitive conclusions'. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.
- So the key thing is how we talk about our models - it is *us* that moves between the small world and the large world.

# Recap {background-color="#40666e"}

## Recap

- If you are working with observational data (as is common in criminology), coefficients from a GLM may well be biased
- Quantitative bias analysis describes three secnarios where we may want to be critical about our results:
    - Measurement error
    - Selection effects
    - Unmeasured confounding
- Measurement error has received attention in criminology when analysis recorded crime and victimization counts
- So has selection effects in police data on stops and when analysing court data
- Unmeasured confounding less so
- The potential impact of these three factors depends on your specific data and analysis - there are no general solutions :shrug:

# Practical {background-color="#40666e"}

- Using either the generative story that you came up with in Session One, or the description of the SOI from the first practical, describe the possible effects of:

    -   Measurement error?
    -   Selection effects?
    -   Omitted variables?

Are you concerned about all these? Some more than others? Are there obvious steps you could take to address them?

# Break {background-color="#666666"}

---


# Simulation {background-color="#40666e"}


---

## Simulation

- In the last session we spent time being skeptical about our model coefficients, and looked at some methods which adjust coefficients to account for possible biases in the data.
- Now we are going to translate coefficients into more interesting and informative quantities. This is a great way to make results more informative and accessible [@kingMakingMostStatistical2000].



## Translating coefficients

- In simple (linear) models it is possible to read off a coefficient directly as the quantities that we are interested in. 
- In more complex models, such as generalized linear models, we often want to convert the coefficients to express results in a more accessible way
- In poisson regression the model coefficients are also commonly expressed as rate ratios, by exponentiating the coefficients (we did this already!)


## Translating coefficients: challenges

- These approaches don't scale well with more complicated models, or complicated transformations (McElreath; Gelman and Pardoe)
- A common way to translate model coefficients into more meaningful quantities is to use all the coefficients in the model to calculate predicted values of the outcome
- These are often called ['marginal effects'](https://marginaleffects.com/) (although this terminology is confusing)
- The terminology is confusing in part because there are lots of related but subtly different quantities people call marginal effects (see https://marginaleffects.com/vignettes/get_started.html)
- As a result, I like Gelman and Pardoe's terminology of 'adjusted predictive comparisons'



## Calculating a predicted value

- In the second practical we fit a GLM describing the relationship between violent crime and deprivation which gave us the results
- Intercept = 2.4573632, simd 2020 rank = -0.0003029

- Using our GLM formula, we know that the log of the expected violent crime for a data zone with an SIMD rank of 0 is 2.4573632
- NB no such datazone will exist by definition, but that's okay!
- We can calculate the expected violent crime incidents by exponentiating the intercept, giving us 11.674 violent crimes
- To calculate violent crime in the most deprived datazone in Scotland, we just add 1 * -0.0003029 (the coefficient for SIMD) to 2.4573632, and then exponentiate: 11.670
- To calculate violent crime in the least deprived datazone in Scotland, we add 6976 * -0.0003029 (the coefficient for SIMD) to 2.4573632, and then exponentiate: 1.41

## Calculating predictions

:::: {.columns}

::: {.column width="66%"}


```{r preds}
library(ggplot2)

data.frame(
  simd = seq(1, 6976)
) |> 
  mutate(pred_vio = exp(2.4573632 + simd * -0.0003029)) |> 
  ggplot(aes(x = simd, y = pred_vio)) +
  geom_line()



```


:::

::: {.column width="34%"}
In fact, we can calculate these predictions across the whole range of SIMD

:::

::::



## Uncertainty
- A key challenge is propagating the appropriate uncertainty in our results - especially in more complicated models when predictions might be a function of multiple parameters
- We can use simulation to propagate this uncertainty - and we can apply this approach to any generalized linear model and any Derived Quantity of Interest (DQI) - be it a marginal effect or something else
- The simulation process is described by King et al. (2000) and implemented in the R package clarify (https://github.com/iqss/clarify)
- This approach propagates both the uncertainty in the model's coefficients, and the correlations between the coefficients



---

# But first {background-color="#40666e"}


---

## Simulating random variables: a brief introduction

:::: {.columns}

::: {.column width="66%"}

```{r rnorm_sim_v1}
library(tibble)
library(ggplot2)

var1 <- 
rnorm(
  n = 10000,
  mean = 0,
  sd = 1
)

tibble(
  var1 = var1,
) |> 
  ggplot(aes(x = var1)) +
  geom_histogram()

```

:::

::: {.column width="34%"}
`rnorm()` lets you simulate normally-distributed random variables.

Here we simulate a normally distributed random variable and plot its distribution
:::

::::

---

## Simulating a second variable


:::: {.columns}

::: {.column width="66%"}

``` {r rnorm_sim_v2}
var2 <- 
rnorm(
  n = 10000,
  mean = 1,
  sd = 1
)


tibble(
  var1 = var1,
  var2 = var2
) |> 
  ggplot(aes(x = var1, y = var2)) +
  geom_point() +
  geom_density_2d()


```

:::

::: {.column width="34%"}
Now we add a second variable and plot them together.

We can see that they are uncorrelated (as we would expect)
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}


```{r mvtnorm_sim}
cor_data <- MASS::mvrnorm(
  n = 10000,
  mu = c(0, 0), # mu instead of mean
  Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2, ncol = 2) # Sigma instead of sd
) |> 
  as.data.frame()

cor_data |> 
  tidyr::pivot_longer(cols = c(V1, V2),
                      names_to = "variable",
               values_to = "value") |> 
  ggplot(aes(x = value)) +
  facet_wrap(~ variable) +
  geom_histogram()

```


:::

::: {.column width="34%"}
If we use `mvrnorm()` the resulting simulations can be correlated. Here I set a correlation of 0.9.

`V1` and `V2` are both normally distributed...
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}

```{r cor_graph_2}
cor_data |> 
  ggplot(aes(x = V1, y = V2)) +
  geom_point() +
  geom_density_2d()

```


:::

::: {.column width="34%"}
... but highly correlated. 
:::

::::


---

## Back to our example

:::: {.columns}

::: {.column width="66%"}

``` {r clarify}

simd <- readRDS(url("https://github.com/benmatthewsed/statistical-methods-criminology-slides/raw/master/resources/simd_crime_sim.rds"))


mod_vio_simd_qp <- glm(vio_integer_sim ~ simd2020_rank, 
                      family = "quasipoisson",
                      data = simd)


```


:::

::: {.column width="34%"}
If we fit the model from the second practical again...
:::

::::

---

## Back to our example


:::: {.columns}

::: {.column width="66%"}


``` {r clarify2}

library(clarify)

sims <- sim(mod_vio_simd_qp, n = 1000) 

sim_res <- sim_setx(sims, x = list(simd2020_rank = seq(1:6976)))

plot(sim_res)


```

:::

::: {.column width="34%"}
We can now get confidence intervals around our predictions.

This approach can also be used when we want to look at the effects of multiple variables, and where we want to describe more complex transformations of coefficients...
:::

::::


---

## An advanced example: victimization divides

- [@hunterEquityJusticeCrime2016] wanted to describe how victimization inequality had changed over time
- They used the results of a fitted regression model to calculate a measure they call 'Victimization Divide'
- This measure is defined as:

(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)

---

## Victimization divide

This is analogous to exploring the percentage change in victimization inequality between two comparison years.

---

## Victimization divides

To calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a regression model (specifically a count model) to predict the number of burglary victimization incidents experienced by households in England and Wales in 1993 compared to 20089. They use the coefficients from these models as inputs into the Victimization Divide formula. 

---

## Victimization divides

- Based on this analysis they conclude that burglary victimization inequality had increased for:
  -   single adult households compared to other households
  -   social renters compared to owner occupiers
  -   households without a car compared to those with one car
  -   households leaving their home unoccupied any amount of time on a typical weekday compared to those never leaving the home
  -   households in areas without neighbourhood watch compared to those with the scheme
  -   households earning at least £50,000 per annum compared to those on a £10,000–£29,999 annual income
  -   inner city residents compared to households in rural areas

---

## Victimization divides

- But this analysis used data from the Crime Survey for England and Wales
- So we want to assess the uncertainty in these estimates which come from projecting from sample to population
- We can do this with the King et al. simulation approach


---

## Simulating Victimization Divides

- In this process we:
- Use a fitted regression model to simulate a set of coefficient values from the regression model's variance-covariance matrix (usually at least 1000)
- Calculate the VD for each one these simulated coefficient values

---

## Simulating Victimization Divides

- The draws from the variance-covariance matrix reflect:
1. the uncertainty in each of the regression coefficients (as expressed in their standard errors) and 
2. the correlation between these coefficients (as expressed in the covariance between the coefficients)
- We then just calculate the VD for each draw. This gives us a distribution of VDs which capture the uncertainty in the model's coefficients

---

## Simulating DQIs: worked example

:::: {.columns}

::: {.column width="66%"}
```{r vid_fn, fig.show = 'hide'}
#| echo: true

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```
:::

::: {.column width="34%"}
This is what the VD formula looks like in `R`
:::

::::

---

## Simulating DQIs: Getting ready

:::: {.columns}

::: {.column width="66%"}

```{r vd_prep}
#| echo: true

# load packages

library(MASS)
library(tidyverse)

# reading in data

dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )

# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```
:::

::: {.column width="34%"}
You can find more info on the data and approach [here](https://osf.io/k8j9e/)
:::

::::

---

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

```{r vd_models}
#| echo: true

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))
```

:::

::: {.column width="34%"}
We can fit a simple regression model to the data in each year to calculate the log-odds of being a victim for men and women. In this example I fit a separate model for 2015 and 2020.
:::

::::

--- 

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

Model1 :
```{r vd_ests_2015}
#| echo: true
results_2015 <- 
broom::tidy(model2015) |> 
  mutate(est = exp(estimate))

results_2015
```

Model 2:
```{r vd_ests_2020}
#| echo: true

results_2020 <- 
broom::tidy(model2020) |> 
  mutate(est = exp(estimate))

results_2020

```


:::

::: {.column width="34%"}
Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having __11%__ greater odds of being a victim of crime. In contrast, Model 2 finds that men had a __5%__ greater odds of being a victim of crime than women in 2020 - however this difference does not meet the 95% threshold for statistical significance.
:::

::::

---

## How much has victimization inequality changed?


:::: {.columns}

::: {.column width="66%"}

```{r vd_calc}
#| echo: true
victim_divide(
  base_y1 = results_2015$est[[2]],
  base_y2 = results_2020$est[[2]]
  )
```


:::

::: {.column width="34%"}
Calculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by __52%__ between the two years. 

Victimiztion inequality fell by more than half!
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim}
#| echo: true
# set seed
set.seed(nchar("vict divide") ^ 4)

n_sims <- 1e5

draws_2020 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2020),
    Sigma = vcov(model2020)
  )

draws_2015 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2015),
    Sigma = vcov(model2015)
  )
```


:::

::: {.column width="34%"}
We can pass the models' coefficients and variance-covariance matrices to `mvrnorm` from the `{MASS}` library.

This gives us a set of 10,000 coefficients for each model.
:::

::::


---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_calc}
#| echo: true
draws_2015 <-
  draws_2015 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

draws_2020 <-
  draws_2020 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

# combine the results
sim_dat <-
  tibble(
    vd = victim_divide(base_y1 = draws_2015$est,
                       base_y2 = draws_2020$est)
  )


```


:::

::: {.column width="34%"}
Once we tidy the results up a bit, we can pass these simulated draws to our `victim_divide()` function...
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_res}
#| echo: true
sim_dat |> 
  reframe(
    vds = quantile(vd, c(0.025, 0.5, 0.975)),
    centile = c("2.5%", "50%", "97.5%")
    )


sim_dat |> 
  ggplot(aes(x = vd)) +
  geom_histogram(binwidth = 0.1) +
  scale_x_continuous(limits = c(-2, 2)) +
  geom_vline(aes(xintercept = 0))

```


:::

::: {.column width="34%"}
We can see the 5 and 95 percentiales of the simulated VDs, as well as the histogram of their distribution.

These intervals include zero, so the apparent 52% reduction in victimization inequality would not be 'statistically significant' at the standard level.
:::

::::

---

## Simulation is great

- The beauty of this simulation approach described by King et al (2000) is that it generalizes to __any DQI__ that is a function of the model's parameters, such as VD, and to __any GLM__. 
- Say that instead of the VD were interested in the absolute difference in predicted victimization rates after controlling for other factors (like a marginal effect)
- Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating the model's uncertainty into our estimates

---

## Simulation isn practice

- In the second example we did the simulation by hand. This is useful as a way to understand the method, but whilst it's good to know how this works, in practice there are R packages which can do this for us. 
- One good option is [clarify](https://cran.r-project.org/web/packages/clarify/index.html), as used in the first example
- There is inherent uncertainty in simulation results, so it's crucial to set the seed for the random number generator so you get the same results every time. 
- You also need to run enough simulation draws to get a good estimate of the uncertainty. The default for clarify is 1000 draws.
- More simulations are always better, but will take longer - so there is a pragmatic aspect to how much time and computer resource you have to run simulations.
- In general I would recommend using a package like clarify if you conduct this kind of simulation. It's likely that professionally developed and maintained software will be less error-prone and more efficient than writing your own!

---

## Practical limitations and alternatives

- Simulations can be very time consuming! Especially if your model is complicated and you are making predictions for a lot of cases
- There are other ways to do describe this uncertainty:
  - The ['delta method'])(https://marginaleffects.com/vignettes/uncertainty.html#delta-method) uses calculation, not simulation, but relies on a Normal approximation which may not hold (https://iqss.github.io/clarify/#introduction). This is the default method used by the [marginaleffects](https://marginaleffects.com/) package

---

## Conceptual limitations

- Simulation expresses the uncertainty captured _by our model parameters_
- But remember that our model is wrong!! (Greenland)
- Our model still makes a load of assumptions - basically that we've fit the right model and that we had the right data.
- As we discussed in the previous section, we might also want to be skeptical of our model parameters themselves if we are worried about bias


---

## Practical


# Break {background-color="#666666"}

---

# Ethics

## General ethical principles in social science research

- In research ethics and governance there is a lot of discussion about informed consent, not disclosing personal information and so on.
- Also specifically in quantitative work, ethical issues around workflow and 'questionable research practices'
- This is not a general treatment of ethics in social science research
- We're going to focus on the ethics of framing our research and how we present our results

## Framing: The tyranny of the means

- The typical focus of regression results is group average effect
- Even if we have a statistically significant difference between two groups (say, people who live in high deprivation areas versus other areas) in the count of offences recorded by the police, on its own this doesn't necessarily tell us about how likely any individual member of those groups is to offend
- Focusing only on group averages can imply that all members of the group are the same

- Causal quartets?

## Causal quartets

![From @gelmanCausalQuartetsDifferent2023](images/causal-quartets.png){width="90%"}

## Causal quartets

- So what? [@gelmanCausalQuartetsDifferent2023]
- People with the same observable characteristics of your independent variables may have very different values of the outcome
- "Variation among people is relevant to policy (for example, personalized medicine) and understanding (for example in psychology)"
- "Variation across situations is relevant when deciding what “flavor” of treatment to do, for example with dosing in pharmacology or treatment levels in traditional agricultural experiments."


## Same observables, different outcomes

![](images/qlp-results-figure.png){fig-alt="Estimated Adult Conviction Trajectory Probabilities by Sex, Indigenous Status and Childhood System Contact."}

## How we described the figure

"Our results show substantial variation in adult conviction trajectories within sex and Indigenous status groups for people with identical observable characteristics, as well as between our four demographic groups... Even though we see a strong association between Indigenous status, sex and membership of the High/Persistent conviction trajectory (see Appendix 4), it would be wrong to assume that all Indigenous men with multiple childhood system contact end up with High/Persistent adult conviction trajectories—our model estimates that almost half of Indigenous men would _not_ end up with a high-rate, persistent adult conviction trajectory even with this substantial childhood adversity."

https://link.springer.com/article/10.1007/s40865-022-00204-z


## Framing: the importance of language

More conceptually, it's important to think about how we frame the results of any analysis.

Three visualizations from [Data Feminism](https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)

-   language use

-   providing necessary context?

-   deficit narrative

## Stereotyping

"...a narrative that reduces a social group to negative stereotypes and fails to portray them with creativity and agency." (https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)

https://data-feminism.mitpress.mit.edu/pub/czq9dfs5#ndayi2fa1pk

-   the description of your charts is theoretically informed

The value of this exercise is not to say that any of these framings is 'right' (although for the reasons Klein and D'Ignazio outline we might find some preferable to others), but that they reflect different theoretical positions, and give different emphases to contextual factors - factors outside our datasets.

## Ethical responsibilities

"Placing numbers in context and naming racism or sexism when it is present in those numbers should be a requirement—not only for feminist data communication, but for data communication full stop."

Data Feminism is mostly aimed towards data scientists and data journalists, not academics. Does this change how we should view their recommendations?

(the original study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4539829/)

- Our analysis exists in a context of (Tilley) durable inequalities


## An example of theoretical framing: community loss

- @simesPunishingPlacesGeography2021 gives a good example of how we may want to come up with theoretically informed measures, or theoretically re-frame measures. Simes analysed imprisonment data from the state of Massachusets in the USA, including spatial regression of prison admission rates and how these relate to "racial demographics, social and economic disadvantage, arrest rates, and violent crime" [@simesPlacePunishmentSpatial2018a].

- As part of this analysis Simes suggests that the cumulative years sentenced to residents of a particular neighbourhood be thought of as 'community loss'. This is not (just?) an indicator of individual punishment histories, but reflects the chronic and long-term exposure to loss due to imprisonment in different neighbourhoods. This highlights the effects of imprisonment on the communities in which people who end up in prison lived prior to their imprisonment, rather than just focusing on the people who are currently in prison.

## The story of our results should reflect the story of the data

- Kotze criticises the 'international crime drop' literature for being too focused on the Global North
- International Crime Victims Survey has limited international coverage (despite the name)
- The problem is making the link between the small world of the model and the large world


---

# Practical


# Overview

## Overview

- Whew! We've covered a lot of ground today
- The key idea of this workshop is that the data provenance story should influence our analysis and the story we tell about our results
- We looked at common data provenance stories of police recorded crime and victimization surveys
- We talked about GLMs as a common way of telling stories about the small world of our data
- We looked at how we might want to critique the stories our GLMs are telling, using tools from quantitative bias analysis
- And how to use simulation approaches to tell more meaningful stories from our results
- We discussed some ethical issues that come up when we present our results

## Where next?

- Data provenance: Over to you!
- GLMs (and beyond): (Statistical Rethinking)[https://github.com/rmcelreath/stat_rethinking_2024]
- Quantitative Bias Analysis: :shrug: Maybe (_Quantitative Assessment of Systematic Bias: A Guide for Researchers_)[https://journals.sagepub.com/doi/10.1177/00220345231193314] or [@kawabataQuantitativeBiasAnalysis2023]?
- Simulating meaningful quantities from results: (`{clarify}`)[https://iqss.github.io/clarify/] and (`{marginaleffects}`)[https://marginaleffects.com/]
- Ethics: (Data Feminism)[https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3]

---

## References