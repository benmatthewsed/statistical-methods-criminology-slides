---
title: "Statistical Methods for Criminology"
author: "Ben Matthews"
execute: 
  echo: true
format:
  revealjs:
    theme: solarized
    smaller: true
    scrollable: true
---

## Welcome!

- Who we are
- [workshop outline](https://github.com/benmatthewsed/ui-data-visualization-course/blob/master/course_outline.md)
- [Code of conduct](https://github.com/benmatthewsed/ui-data-visualization-course/blob/master/code_of_conduct.md)
- [Reference texts](https://github.com/benmatthewsed/ui-data-visualization-course/blob/master/course_outline.md#course-texts)

---

## Learning outcomes

By the end of the workshop you will:

- Understand the main forms of criminological data 
- Understand how data about crime and victimization can be understood using the General Linear Model (GLM)
- Be aware of issues that can arise in interpreting GLM coefficients due to measurement error, selection bias and omitted variables
- Be aware of ways to transform GLM coefficients into meaningful Derived Quantities of Interest (DQI)
- Understand ethical issues around how results from statistical methods fit to criminological data are discussed


---

## Course outline

_add this here_


---

## Points of order

- __Ask questions whenever__ `r emoji::emoji_find("thumbsup")$emoji` You can do this through the Teams chat facility __{demo this now}__
- We've structured the sessions with regular breaks, but __if you need to leave just leave__!
- Materials will live online ([slides](https://benmatthewsed.github.io/statistical-methods-criminology-slides/statistical_methods_criminology.html#/title-slide),) so you can access them any time
- We want this time to be as useful for you guys as possible, so __please let us know anything we can improve on__ between now and next week's session


---

## `R` set-up

- You can follow along with the materials in a local installation of `R` and `RStudio` on your own computer


---

## Who you are

- Your __research interests__
- __Why__ this course?
- What do you want to __achieve__?


---

class: center, middle, inverse

## Before we begin...

---

## Statistical analysis and stories

- This workshop is _also_ about stories. Specifically:
  - The story of how the data in your spreadsheet came to exist
  - The story you tell about these data based on statistical analysis
- I want to convince you that the first of these two stories should filter through into every decision you make during analysis, and so should determine the second
- You need to know how your data came about to be able to analyse it properly

---

class: center, middle, inverse

# Session One: Types of criminological data

---

## Types of criminological data

- Criminology as a 'rendezvous discipline' as David Downes said
- Criminological data could be
  - Adminsitrative data from the justice system (police, courts, prisons, probation... etc)
  - Secondary survey data (e.g. victimization, offending, fear of crime... etc)
  - Newspaper reports (e.g. collations of stories on police use of force or homicides)
  - Social media data (e.g. fear of crime)
  - 'Digital trace' data from darknet drugs transactions
  - ...

---

## Key types of criminological data

- Here we focus on police recorded crime and victimization surveys as common forms of criminological data
- If you are interested in another form of data, please do ask!

---

## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
- How has the data been collected and/or by whom or by what?
- What/who is included and what/who is excluded?
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
- What are the relevant definitions and concepts that govern the data/data collection?"

[@8347e95e012d4212a5ee429a18ee592e, p228-229]

---

## How does an event become a crime statistic?

![Recorded crime](resources/recorded_crime_flowchart.pdf){style="transform: rotate(90deg);"}

## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of police reporting activity on crime levels
- How has the data been collected and/or by whom or by what?
    - https://www.gov.scot/publications/recorded-crime-scotland-2023-24/pages/17/
- What/who is included and what/who is excluded?
    - Excludes: crimes not reported and for which there was insufficient evidence
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Depends? National statistics are routine collections, there are other data though https://www.law.ed.ac.uk/sites/default/files/2022-08/FPN%204th%20report%20-%20FINAL.pdf
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - Recorded crime data bulletin - yes, cleaned and standardized
- What are the relevant definitions and concepts that govern the data/data collection?"
  - The current [Scottish Crime Recording Standard](https://www.gov.scot/publications/scottish-crime-recording-standard-crime-recording-counting-rules-2/) is 550 pages long (!). I have not read it (and don't intend to!)

---

## Implications of this story

- Reporting on crime levels - incentives to reduce figures?
    - https://osr.statisticsauthority.gov.uk/publication/the-quality-of-police-recorded-crime-statistics-for-england-and-wales/
- Excludes incidents not reported (see dark figure of crime)
- Figures represent (unknown?) mix of behavioural and system effects
  - Urban areas have high police recorded crime rates because ...
      - that's where crime is?
      - that's where poverty and disadvantage are?
      - that's where disproportionate surveilance and punishment are?

---   

## Implications for analysis: Population or super-population?

- One reason people use inferential statistical models is to generalize from the data they have to a wider population. With recorded crime data, is this the whole population?
-  outline two possible approaches to conceptualizing police recorded crime data: a population or superpopulation approach [@verlaan2024use]
- Population approach: if you have police recorded crime data for Scotland in 2023 the data you have is all you could ever have. You don't need to fit statistical models to generalize from the data you observe to a wider population because *there is no wider population*.
- Superpopulation approach: you can think of Scotland in 2023 as 'drawn' from from the population of the UK, or Europe, or the whole planet. Alternatively, data from Scotland from 2023 may be seen as a sample from Scotland from 2023 and 2024, or 2023-2030 and so on. You might also want to construct a counterfactual of what crime would have been like in Scotland in 2023 if unemployment was 5% lower or 5% higher Gelman (2011)

---

## Dangers of inferential statistics with recorded crime data

- It wrongly leads people to assume that their results are generalizable [@verlaan2024use]
- It can lead people to undervalue actual observed differences in their data. For example, researchers may deny that an association between two variables exists in their dataset if it is not statistically significant

---

## Dangers of _not_ using inferential statistics with recorded crime data

- confidence intervals should be reported even when describing statistics from the full population, especially if the results are to be used to make predictions or inform policy. Importantly, even if _you_ are not be interested in prediction or policy-making, you can't control how others will use your results [@d.redelingsWhyConfidenceIntervals2012]
- Different sized populations (e.g. local authorities) have different levels of variability [@spiegelhalterFunnelPlotsComparing2005]. Crime rates in small places will be more volatile - and so have more uncertainty - year-on-year than those from large areas


---

## Survey data

- In response to known issues with measuring levels of crime with administrative data, since the 1980s criminologists in some parts of the world have been surveying the public to ask about their levels of victimization.
- Scottish Crime and Justice Survey​
- Crime Survey for England and Wales​
- Equivalents in other countries, primarily in Western Europe, North America and Australasia ​
- Smaller geographical scale surveys such as The Mayor's Office for Policing And Crime (MOPAC) Survey in London

---

## Survey data: benefits

- Typically ask people about their experiences of victimization in the last year​
- Can measure crime that isn’t reported to the police​
- Usually don’t ask about people’s offending behaviour​
- Good for measuring common crimes, bad for measuring rare crimes

---

### Answering our key questions

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of government reporting on experiences and attitudes of criminal justice
- How has the data been collected and/or by whom or by what?
    - Collected by professional survey agencies
- What/who is included and what/who is excluded?
    - Random sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)
  - For those who agree, one adult (age 16 or over) in each household is randomly selected for interview​
- Therefore, *by design* no:​
  - Children​
  - Homeless people​
  - People living in communal establishments (e.g. students, people in prison)
  - Also in practice may exclude those in 'fragile, disjointed households' [@carr-hillMissingMillionsMeasuring2013]
  - For SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​

But this excludes people, students, people in prison, and those living in refuges. However, “Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”
  
---
  
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Routine reporting
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - When it gets to you it has usually been validated. Some information is also restricted, such as 'capping' counts of victimization - we'll talk about this later...
- What are the relevant definitions and concepts that govern the data/data collection?"
  - You can read the question wording online! But... the questions included can changes over time (https://www.sccjr.ac.uk/wp-content/uploads/2024/02/group-two_hackathon-output.pdf)
  
  
---
  
## Implications for analysis: weighting

- We almost certainly do want to do inferential statistics!

- Surveys are very rarely random samples; most come with weights
- Use the weights for descriptives to gross up to national populations
- Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model - it may depend on the specifics of your survey
- But, weights will only help adjust the data you see towards the target population in the survey design frame. Weighting cannot adjust for populations who are excluded from the survey by design.



--- 

# Recap:
# Data provenance determines what types of analysis are appropriate for a given aim
# Recorded crime and victimization surveys exclude some incidents/population groups by design
# There are pros and cons of using inferential statistics with recorded crime data


---


# Practical

- Describe SOI
- Give examples from my thesis of being inconsistent

- Do I need significance testing here?

- What's the story of *your* crime data?



---
class: center, middle, inverse

# The general linear model


---

## Large worlds and small worlds

- Our spreadsheets and model coefficients can only summarize the `small world` for us, and omit some of the complexity of the `large world`
- In this session we'll overview the most common[^3] way in which criminologists understand the 'small world' of their spreadsheets: the general linear model (GLM)
- But be warned, as soon as we want to understand the large world we can run in to problems if all we focus on are the coefficients..

---

## A disclaimer

- If you are taking this course I assume that you have some familiarity with linear models of some description. This session is therefore mostly a refresher.

---


## A(nother) disclaimer

- You could be possible to spend months and months studying GLMs and their various extensions. Some specific flavours of model that we *don't* cover, but which may be useful for your own work include:
- Mixed/multilevel/hierarchical/etc (see e.g. https://www.cmm.bris.ac.uk/lemma/)
- Additive models (GAMs) (which are less common in criminology, but are very useful; https://noamross.github.io/gams-in-r-course/)
- 'bespoke' models (very infrequently used in criminology to my knowledge, https://betanalpha.github.io/assets/case_studies/generative_modeling.html, but see e.g. https://josepinasanchez.uk/wp-content/uploads/2018/09/bsc-presentation.pdf)

## Anatomy of a GLM

-   Stochastic (or random) component
-   Systematic component
-   Link function that converts between the parameter estimates and the form of the outcome (we'll say more about this later)

## Anatomy of a GLM

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## The linear model

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## Logistic regression

For logistic regression we have:

$$
\begin{align*}
y_i \sim & {Binomial} (n, p_i) \\
{logit(p_i)} & = \alpha + \beta (x_i),
\end{align*}
$$ 

for logistic regression, $n$ = 1, and we are just interested in modelling $p_i$, the probability of the outcome. The nice thing about this model formulation is that the ${logit}$ link function makes sure that all the probabilities the model estimates are between 0 and 1.


## Count data

Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims. 
(Whilst we may see crime data be re-expressed as *rates* per 1,000 population, before this they are counts.)

## Poisson model

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The coefficients from Poisson models (which range from $-\infty$ to $\infty$) are converted to be predicted counts that are non-negative integers via the $\log$ link function. This lets us have coefficients which can take any value (-2! 0.5!), but then convert these to counts as our count outcome demands.

## why not just use a linear model?

If you really want to you can just use a linear model for count data. This will still give you an accurate model of the mean of your outcome. However, there are two main problems with this approach.

## Negative counts?

First, if you have small counts (say, if you were modelling homicides in Scotland) the uncertainty in the mean estimate may give you a confidence interval below zero:

```{r}
library(dplyr)

set.seed(12346)

n_draws <- 1e3

dat <- 
data.frame(
  y = rpois(n = n_draws,
            lambda = 0.001) # draw from poisson distribution with mean 0.001
)

dat |> 
  count(y)

lm(y ~ 1, data = dat) |> # fit intercept-only model with normal outcome
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error)


glm(y ~ 1, # fit intercept only model with poisson outcome
    family = "poisson",
    data = dat) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low))

```

Here the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive). 


## Non-constant variance?

Second, the standard linear model assumes constant variance. But in practice we probably want more variance for larger counts. 

See figure at https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html

## Interpretting coefficients

- Per UCLA OARC: "for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant."

- The most straightforward way to interpret coefficients from Poisson models is to exponentiate the coefficient value. This converts the beta coefficient into an Incident Rate Ratio (https://stats.oarc.ucla.edu/stata/output/poisson-regression/)


---

### Problems with Poisson: over-dispersion

-   poisson assumes mean and variance are the same (in that there is only one term in the model, $\lambda$, which describes both the average and the variability around the average)

- This is quite a brittle assumption though, and there's no reason that real-life data has to obey it. So what? If your data are over-dispersed (the conditional variance exceeds the conditional mean), you will get standard errors that are too small.
- p-values are too optimistic
- need some way to account for over-dispersion

---

### Solutions?

##### What do to? about over-dispersion

- If all you care about is your standard error being too small you can use quasi-poisson
- This gives same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it's also faster than the standard alternative...
- The negative binomial is a flavour of statistical model ^[okay, strictly speaking there are different varieties of negative binomial model. So... flavours? [@hilbeModelingCountData2014]] for count data which has an extra "dispersion" parameter, which allows you to model the over-dispersion directly
- People disagree as to which is preferable



- Example of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf


---

## Practical

In this practical we'll fit some count models in R


---

class: center, middle, inverse

# Measurement error, selection and confounding


---

## From small world to large world?

- There are lots of reasons we may be skeptical about applying conclusions drawn from 'small world' of our data and model to the 'large world' we live in
- In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models
- Again, books and books have been written about each topic, so this will only offer a brief overview of each, with links to further reading for those who want to know more.

---

## Are coefficients sufficient?

- We should be skeptical about the numbers in our spreadsheets and the coefficients that summarise them
- The numbers may be (predictably) inaccurate (measurement error);
- they may omit some variables that we theoretically think are important (confounding), or;
- they omit some people/cases we care about (selection effects)
- These are problems that are hard to solve based on the data we observe alone
- There is a large literature in epidemiology stu

---

class: center, middle, inverse

# Act One: Measurement error

---

## Measurement error

- Measurement error is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet
-  GLM assumes no measurement error
- In practice we know criminological data are likely to be measured with some degree of error. For example, we know that police recorded crime data is not a perfect measure of the amount of crime 'out there' in society. As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.


---

## Measurement error in practice: police recorded crime

-   From our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data.
-   So if we are interested in, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an *under* count.
-   The many years of work comparing crimes recorded to the police with victimization surveys - the 'dark figure of crime' - attests to this.

---

## `rcme`

Pina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.

> Work through their example here? https://osf.io/preprints/socarxiv/sbc8w

Open questions - what about survey data? Models other than linear models? What about measurement error in independent variables?

- At the moment seems like this focuses on measurement error with recorded crime as the dependent variable

---

## Example Two: Measurement error in victimizaton survey data

-   Historically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete. In 2019, ONS said that "Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way." (https://doc.ukdataservice.ac.uk/doc/7280/mrdoc/pdf/7280_csew_improving_victimisation_estimates_2019.pdf)

-   However, it also means that people who experienced more than five incidents of a particular 'series' crime type did not have their data accurately recorded

-   This was particularly important for women's reporting of violent victimization (Walby et al. 2015) - women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year. A second measurement error issue came from the '97 code' - the option to report the number of incidents experienced as '96/too many to count'. Instead, based on the domestic violence literature Walby and colleagues propose using an estimate of 60 incidents for those who report the 97 code.

- This would bias estimates of total crimes , but not prevalence (the number of victims)

-   Sometimes it's possible to use uncapped data

---

## Adventures in measurement error

- The two examples we've looked at have focused on measurement error in the outcome (recorded crime and victimization)
- But it also matters where in your model the measurement error manifests (/manifests more)
- If you have error in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results (meaning that they are less likely to be statistically significant).
- Measurement error in your key independent variable may bias your regression coefficients *downwards*, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. 
- But if there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to 'under controlling' - and finding statistically significant coefficients where there are none. 
- It's quite context dependent - so you need to know the likely source of error in your specific dataset
- Other than in simple scenarios *we may not know what impact it is having*. Uh oh!


## Residual or unmeasured confounding

Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we don't have a measure for this
- Famous example of smoking and lung cancer
- There are ways you can try to quantify this discussed in the field of quantitative bias analysis. I have not seen these methods used in criminology very much
- One informal approach to unmeasured confounding is to claim that your measures are good enough to make it not a problem

-   because we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).

*The tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven't measured*, although Peto suggests that these in practice are hard to distinguish

*need some criminological examples here*


Probably the biggest limitation of our study is that the IPTW modeling
approach we adopted assumes no unmeasured covariates linked to both
treatment and outcome. In practice, the criterion of having no unobserved
confounding is impossible to verify—the data in any observational study
provide no definitive information (Robins, Hernán, and Brumback, 2000).
As discussed above, however, we tried to counteract this limitation by
exploiting what we believe are rich individual baseline data and timevarying covariates over the full life course in order to model the
propensity to marriage. It is hard to imagine what the missing time-stable
or time-invariant covariates are that would overcome the magnitude and
robustness of results. From IQ to the cumulative history of both the
outcome and the treatment, we accounted for 20 baseline covariates and
approximately a dozen time-varying confounders measured from widely
varying sources—many of which predict the course of marriage as
theoretically expected (table 1). 



We thus argue that omitted confounders would have to be implausibly
large to overturn the basic results obtained under a number of different
model specifications and assumptions.22

22 A formal sensitivity analysis (see Robins, 1999: 167–73) is beyond the scope of the
current paper. Moreover, such analyses require assumptions about the magnitude,
direction, and functional form of potential biases that ultimately raise more
questions than they answer

(https://psycnet.apa.org/record/2008-07491-001)

- It's true that you have to make assumptions about the unmeasured confounding to know the impact on your results, and so it's necessarily speculative

- But this is possible!

"However, increasing the number of covariates is hardly
a persuasive approach to ruling out potentially important confounders,
as it is unlikely that one can adequately measure all such confounders"

- from their study they find:

```{r}

# from table 2 in Sampson, Laub and Wimer, p 491 
# https://scholar.harvard.edu/files/sampson/files/2006_criminology_laubwimer_1.pdf

library(EValue)

evalues.RR(est = 0.572, lo = 0.511, hi = 0.640)
  bias_plot(0.572, xmax = 10)

```

From this, the unmeasured confounder would have to be associated with an almost three-fold increase in the risk of offending, and must be almost three times more prevalent in those married than those not married, to explain the observed risk ratio.

The question then becomes... how plausible is this?



See https://cran.r-project.org/web/packages/EValue/vignettes/multiple-bias.html

 https://link.springer.com/book/10.1007/978-3-030-82673-4

"The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding"

-   so this is based on the idea tat we can identify an "implausibly strong" confounder, which is reasonable.

-   One approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.


## Selection effects

Selection bias arises when there are people who we would have liked to observe in our study but we don't observe them, and this lack of observation is related to their characteristics. Put another way, we can think of selection bias as affecting the rows of our dataset - there are some rows that are missing that we would like to see.

[@greenlandSensitivityAnalysisBias2014]


In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset *would* know this.

Even if we *do* know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we'd need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)


Imagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.

For example, we might want to know if people from minority ethnic backgrounds more likely to be arrested after a stop than people from white backgrounds (Knox et al)

Police collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?

The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”

We're going to hear a lot about 'strong and untestable assumptions'.

“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”

\[INSERT FIGURE 2 FROM KNOX\]

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.

Others [@gaeblerCausalFrameworkObservational2022]suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).

## Solutions?

Knox et al. (2020) suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify *causal* relationships *there is no general solution* that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records. The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.

---

## Should I do a quantitative bias analysis?

- It depends
- ... so you only need to bother with this stuff if you 'claim to offer near-definitive conclusions'. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.
- So the key thing is how we talk about our models - it *us* that moves between the small world and the large world.

---

## Practical

-   We're going to revisit the generative stories that you came up with in Section One. Is there possible:
    -   Measurement error?
    -   Selection effects?
    -   Omitted variables?
-   Write a vignette describing some results and then critique?



---
class: center, middle, inverse

# Simulation


---

## Simulation

- In the last session we spent time being skeptical about our model coefficients, and looked at some methods which adjust coefficients to account for possible biases in the data.
- Now we are going to translate coefficients into more interesting and informative quantities. This is a great way to make results more informative and accessible [@kingMakingMostStatistical2000].

---

# But first


---

## Simulating random variables: a brief introduction

:::: {.columns}

::: {.column width="66%"}

```{r rnorm_sim_v1}
library(tibble)
library(ggplot2)

var1 <- 
rnorm(
  n = 10000,
  mean = 0,
  sd = 1
)

tibble(
  var1 = var1,
) |> 
  ggplot(aes(x = var1)) +
  geom_histogram()

```

:::

::: {.column width="34%"}
`rnorm()` lets you simulate normally-distributed random variables.

Here we simulate a normally distributed random variable and plot its distribution
:::

::::

---

## Simulating a second variable


:::: {.columns}

::: {.column width="66%"}

``` {r rnorm_sim_v2}
var2 <- 
rnorm(
  n = 10000,
  mean = 1,
  sd = 1
)


tibble(
  var1 = var1,
  var2 = var2
) |> 
  ggplot(aes(x = var1, y = var2)) +
  geom_point() +
  geom_density_2d()


```

:::

::: {.column width="34%"}
Now we add a second variable and plot them together.

We can see that they are uncorrelated (as we would expect)
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}


```{r mvtnorm_sim}
MASS::mvrnorm(
  n = 10000,
  mu = c(0, 0), # mu instead of mean
  Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2, ncol = 2) # Sigma instead of sd
) |> 
  as.data.frame() |> 
  ggplot(aes(x = V1, y = V2)) +
  geom_point() +
  geom_density_2d()
```

:::

::: {.column width="34%"}
If we use `mvrnorm()` the resulting simulations can be correlated.

Here I set a correlation of 0.9.
:::

::::





---

## Translating coefficients

- In simple (linear) models it is possible to read off a coefficient directly as the quantities that we are interested in. 
- In more complex models, such as generalized linear models, we often want to convert the coefficients to express results in a more accessible way
- In poisson regression the model coefficients are also commonly expressed as rate ratios, by exponentiating the coefficients

---

## Translating coefficients: challenges

- These approaches don't scale well with more complicated models, or complicated transformations (McElreath; Gelman and Pardoe)
- A key challenge is propagating the appropriate uncertainty in the results
- But we can use simulation to propagate this uncertainty - and we can apply this approach to any generalized linear model and any Derived Quantity of Interest (DQI)
- The simulation process is described by King et al. (2000) and implemented in the R package clarify (https://github.com/iqss/clarify)

---

## An example: victimization divides

- [@hunterEquityJusticeCrime2016] use the results of a fitted regression model to calculate a measure they call 'Victimization Divide'
- This measure is a way to describe how victimization inequality has changed over time
- This measure is defined as

(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)

---

## Victimization divide

this is analogous to exploring the percentage change in victimization inequality between two comparison years.

---

## Victimization divides

To calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a regression model (specifically a negative binomial model) to predict the number of burglary victimization incidents experienced by households in England and Wales in 1993 compared to 20089. They use the coefficients from these models as inputs into the Victimization Divide formula. 

---

## Victimization divides

- Based on this analysis they conclude that burglary victimization inequality had increased for:
-   single adult households compared to other households
-   social renters compared to owner occupiers
-   households without a car compared to those with one car
-   households leaving their home unoccupied any amount of time on a typical weekday compared to those never leaving the home
-   households in areas without neighbourhood watch compared to those with the scheme
-   households earning at least £50,000 per annum compared to those on a £10,000–£29,999 annual income
-   inner city residents compared to households in rural areas

---

## Victimization divides

- But this analysis used data from CSEW
- So we want to assess the uncertainty in these estimates which come from projecting from sample to population
- We can do this with simulation


---

## Simulating DQIs

- In this process we:
- Use a fitted regression model to simulate a set of coefficient values from the regression model's variance-covariance matrix (usually at least 1000)
- Calculate the VD for each one these simulated coefficient values

---

## Simulating DQIs

- The draws from the variance-covariance matrix gives a set of draws which reflect:
1. the uncertainty in each of the regression coefficients (as expressed in their standard errors) and 
2. the correlation between these coefficients (as expressed in the covariance between the coefficients)

---

## Simulating DQIs: worked example

:::: {.columns}

::: {.column width="66%"}
```{r vid_fn, fig.show = 'hide'}
#| echo: true

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```
:::

::: {.column width="34%"}
This is what the VD formula looks like in `R`
:::

::::

---

## Simulating DQIs: Getting ready

:::: {.columns}

::: {.column width="66%"}

```{r vd_prep}
#| echo: true

# load packages

library(MASS)
library(tidyverse)

# reading in data

dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )

# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```
:::

::: {.column width="34%"}
You can find more info on the data and approach [here](https://osf.io/k8j9e/)
:::

::::

---

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

```{r vd_models}
#| echo: true

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))
```

:::

::: {.column width="34%"}
We can fit a simple regression model to the data in each year to calculate the log-odds of being a victim for men and women. In this example I fit a separate model for 2015 and 2020.
:::

::::

--- 

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

Model1 :
```{r vd_ests_2015}
#| echo: true
results_2015 <- 
broom::tidy(model2015) |> 
  mutate(est = exp(estimate))

results_2015
```

Model 2:
```{r vd_ests_2020}
#| echo: true

results_2020 <- 
broom::tidy(model2020) |> 
  mutate(est = exp(estimate))

results_2020

```


:::

::: {.column width="34%"}
Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having __11%__ greater odds of being a victim of crime. In contrast, Model 2 finds that men had a __5%__ greater odds of being a victim of crime than women in 2020 - however this difference does not meet the 95% threshold for statistical significance.
:::

::::

---

## How much has victimization inequality changed?


:::: {.columns}

::: {.column width="66%"}

```{r vd_calc}
#| echo: true
victim_divide(
  base_y1 = results_2015$est[[2]],
  base_y2 = results_2020$est[[2]]
  )
```


:::

::: {.column width="34%"}
Calculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by __52%__ between the two years. 

Victimiztion inequality fell by more than half!
:::

::::

---

## How much has victimization inequality changed?
