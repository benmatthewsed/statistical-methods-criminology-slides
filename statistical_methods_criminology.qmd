---
title: "Statistical Methods for Criminology"
title-slide-attributes:
    data-background-color: "#006938"
author: "Ben Matthews | University of Stirling"
bibliography: references.bib
execute: 
  echo: true
format:
  revealjs:
    theme: [clean.scss]
    smaller: true
    scrollable: true
---

##

![](https://c2.thirdlight.com/file/8/Tb2SsM4TXyBWQHTblABTsjunye/primary-logo.png){width=80%}

![](https://www.ncrm.ac.uk/site-resources/images/webpage-meta-image.jpg)




## Welcome!

- Who we are
- Who you are:
  - Your __research interests__
  - __Why__ this course?
  - What do you want to __achieve__?


## Housekeeping

- Code of conduct
- __Ask questions whenever__ :thumbsup: You can do this through the Teams chat facility __{demo this now}__
- We've structured the sessions with regular breaks, but __if you need to leave just leave__!
- Materials will live online ([slides](https://benmatthewsed.github.io/statistical-methods-criminology-slides/statistical_methods_criminology.html#/title-slide),) so you can access them any time


---

## Learning outcomes

By the end of the workshop you will:

- Understand the main forms of criminological data 
- Understand how data about crime and victimization can be understood using the General Linear Model (GLM)
- Be aware of issues that can arise in interpreting GLM coefficients due to measurement error, selection bias and omitted variables
- Be aware of ways to transform GLM coefficients into meaningful Derived Quantities of Interest (DQI)
- Understand ethical issues around how results from statistical methods fit to criminological data are discussed


---

## Course outline

![](resources/course-outline.jpg)


## Course schedule

10:00 – Welcome
10:15 – Data provenance: Types of criminological data
11:15 – Break
11:30 – Modelling: Telling stories with the general linear model
12:30 – Lunch
13:30 – Q&A
13:40 – Sensitivity analysis and critiquing results: Measurement error, selection and confounding in crime data
14:40 – Break
14:55 – Simulation methods and translating results: Making model coefficients meaningful with simulation
15:55 – Short break
16:05 – Discussing results and ethics
16:45 – Recap and Q&A
17:00 – Depart


---

## Session structure

- In each session I'll present for the first half(ish)
- Then there will be some exercises and group discussion for the second half(ish)

## Practical sessions

- You can follow along with the materials in a local installation of `R` and `RStudio` on your own computer
- You can find the course materials at https://github.com/benmatthewsed/statistical-methods-criminology-slides/
- If you don't have an `R` installation, you can follow along in your browser via posit.cloud [Ben post the link in the chat now!]
- Or you can use the PDFs of the results rather than run them yourself



# About this workshop {background-color="#006938"}

---

## What this workshop is not

- This workshop is not an in-depth guide on how to:
  - fit a particular statistical model, 
  - select which kind of statistical model you should fit
  - use particular statistical software
- These are important topics! And there are a [wealth of training courses](https://www.ncrm.ac.uk/training/) covering them

## What this workshop is not (continued)

- This workshop is also not a comprehensive guide to every issue that can arise analysing criminological data
- Criminology as a 'rendezvous discipline' as David Downes once said[^1]
- You could plausibly be analysing a [randomized control trial](https://link.springer.com/journal/11292), or [spatial data](https://www.annualreviews.org/content/journals/10.1146/annurev-criminol-011419-041423), or [attitudinal questions from a survey](https://journals.sagepub.com/doi/abs/10.1177/1748895817721273) - all of which have specific considerations for statistical analysis - and still be doing criminology

[^1]: As cited in @youngPraiseDangerousThoughts2003


## What this workshop _is_

- Instead, this workshop will give an introduction to a set of issues that are (I hope) general to most (or maybe all) analysis of criminological data
- Some of these issues do not have obvious solutions - but at the end of the workshop you will at least be able to describe and acknowledge them, and the possible impacts they may have on inferences from criminological data
- I will also signpost where you can go to find out more about each of these issues so you can keep exploring those that are most relevant to you


## Course outline

![](resources/course-outline.jpg)



## Statistical analysis and stories

- This workshop is about statistical methods for criminology
- This workshop is _also_ about stories. Specifically:
  - The story of how the data in your spreadsheet came to exist (sometimes called [data provenance](https://betanalpha.github.io/assets/chapters_html/tree_diameter_growth_analysis.html#sec:provenance))
  - The story you tell about these data based on a statistical analysis (i.e. your results)
- I want to convince you that the first of these two stories should filter through into every decision you make during analysis, which determines the story you can tell about your results
- You need to know how your data came about to be able to analyse it properly




# Session One: Types of criminological data{background-color="#006938"}

---

## Types of criminological data

- Criminology as rendesvouz discipline again - varied questions, varied approaches to answering them using varied data
- Criminological data could be:
  - Administrative data from the justice system (police, courts, prisons, probation... etc)
  - Secondary survey data (e.g. victimization, offending, fear of crime... etc)
  - Newspaper reports (e.g. collations of stories on police use of force or homicides) [@roussellDarkFootprintState2022]
  - Social media data (e.g. fear of crime) [@solymosiCrowdsourcingSubjectivePerceptions2018]
  - 'Digital trace' data from darknet drugs transactions [@cunliffeNonmedicalPrescriptionPsychiatric2019]
  - ...

---

## Key types of criminological data

- Here we focus on admistrative data (mostly police recorded crime) and victimization surveys as the most common forms of criminological data
- If you are interested in another form of data, please do mention this in the discussion sessions!

## Data provenance as the story of your data

- Data provenance: how specific numbers in your spreadsheet came to be in your spreadsheet
- Sometimes well documented (in the case of e.g. national surveys)
- Sometimes not so well documented!

## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
- How has the data been collected and/or by whom or by what?
- What/who is included and what/who is excluded?
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
- What are the relevant definitions and concepts that govern the data/data collection?"

[@8347e95e012d4212a5ee429a18ee592e, p228-229]



## Example One: How does an event become a crime statistic?

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Reconviction rates could be used to rank performance across different local
authorities. However, there is an inherent problem in using this approach since
it implicitly assumes that a difference in reconviction rates reflects a ‘real’
difference between local authorities. In reality, all systems within which these
local authorities operate, no matter how stable, will produce variable outcomes
in the normal run of events. In particular, outcomes in local authorities with
smaller sized populations tend to vary more than those in local authorities with
larger populations. The question we need to answer is therefore: Is the
observed variation more or less than we would normally expect? "

:::

::::


## Questions to ask any type of data

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of police reporting activity on crime levels
- How has the data been collected and/or by whom or by what?
    - https://www.gov.scot/publications/recorded-crime-scotland-2023-24/pages/17/
- What/who is included and what/who is excluded?
    - Excludes: crimes not reported and for which there was insufficient evidence
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Depends? National statistics are routine collections, there are other data though https://www.law.ed.ac.uk/sites/default/files/2022-08/FPN%204th%20report%20-%20FINAL.pdf
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - Recorded crime data bulletin - yes, cleaned and standardized
- What are the relevant definitions and concepts that govern the data/data collection?"
  - The current [Scottish Crime Recording Standard](https://www.gov.scot/publications/scottish-crime-recording-standard-crime-recording-counting-rules-2/) is 550 pages long (!). I have not read it (and don't intend to!)

---

## Implications of this story

- Reporting on crime levels - incentives to reduce figures?
    - https://osr.statisticsauthority.gov.uk/publication/the-quality-of-police-recorded-crime-statistics-for-england-and-wales/
- Excludes incidents not reported (see dark figure of crime)
- Figures represent (unknown? unknowable?) mix of behavioural and system effects
  - For example, urban areas have high police recorded crime rates because ...
      - that's where crime is?
      - that's where poverty and disadvantage are?
      - that's where (disproportionate) surveillance and punishment are?

---   

## Implications for analysis: Population or super-population?

- One reason people use statistical models is to generalize from the data they have to a wider population that they care about
- This is one use of statistical significance testing, p-values and all that good stuff
- With recorded crime data, is this the whole 'population' of crime?
- @verlaan2024use outline two possible approaches to conceptualizing police recorded crime data: a population or superpopulation approach
- WARNING statistical jargon ahead!


## Implications for analysis: Population or super-population?
- _Population approach_: if you have police recorded crime data for Scotland in 2023 the data you have is all you could ever have.
  - You don't need to fit statistical models to generalize from the data you observe to a wider population because *there is no wider population*.
  - All you need are descriptive statistics
- _Superpopulation approach_: you can think of Scotland in 2023 as 'drawn' from from the population of the UK, or Europe, or the whole planet and want to generalize to this wider population. 
  - Alternatively, data from Scotland from 2023 may be seen as a sample from Scotland from 2023 and 2024, or 2023-2030 and so on. 

---

## Implications for analysis: Population or super-population?

- There is no right or wrong answer really
- But this framing will (or should at least) determine what subsequent analysis you do
- Unfortunately there are pitfalls either way...



## Dangers of inferential statistics with recorded crime data

- @verlaan2024use outline two compelling problems when using inferential statistics with recorded crime data
- 1. It wrongly leads people to assume that their results are generalizable
  - It's a common fallacy that a 'statistically significant' result means that a finding is 'true'
- 2. It can lead people to undervalue actual observed differences in their data
  - For example, researchers may deny that an association between two variables exists in their dataset if it is not statistically significant (Also a common fallacy that a non-statistically significant result means that a finding is not 'true')

---

## Dangers of _not_ using inferential statistics with recorded crime data

- Others, emphasise dangers of not using inferential statistics with administrative data
- Epidemiologists argue that confidence intervals should be reported even when describing statistics from the full population, especially if the results are to be used to make predictions or inform policy. Importantly, even if _you_ are not be interested in prediction or policy-making, you can't control how others will use your results [@d.redelingsWhyConfidenceIntervals2012]
- If we want to make comparisons between different areas, places different sized populations (e.g. local authorities) have different levels of variability [@spiegelhalterFunnelPlotsComparing2005]. Crime rates in small places will be more volatile - and so have more uncertainty - year-on-year than those from large areas

---

## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Reconviction rates could be used to rank performance across different local
authorities. However, there is an inherent problem in using this approach since
it implicitly assumes that a difference in reconviction rates reflects a ‘real’
difference between local authorities. In reality, all systems within which these
local authorities operate, no matter how stable, will produce variable outcomes
in the normal run of events. In particular, outcomes in local authorities with
smaller sized populations tend to vary more than those in local authorities with
larger populations. The question we need to answer is therefore: Is the
observed variation more or less than we would normally expect? "

:::

::::



## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"The funnel plot is a simple
statistical method that takes into account the variability of different sized
populations and so highlights whether there are differences that may be
attributed to some other special cause... The plot takes into account the increased variability of the local
authority groups with smaller populations, where a small increase in the
number of reconvictions may lead to a large percentage change in the
reconviction rate.

:::

::::




## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-01.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Rates for local authority groups which lie inside the funnel are not significantly different from the national rate, and we can then usefully
focus on possible explanations for rates which deviate significantly from the
national figure.

Whilst this
is useful for highlighting that there are practical differences in reconviction
rates between each local authority group, even after taking into account
differences in population sizes, it does not allow us to identify if this disparity is
due to variation in the characteristics of offenders in each area or a variation in
practices between different local authority groups. Different offender
characteristics between local authority groups could include: age, sex, crime,
disposal, deprivation, etc.

:::

::::


## Dangers of _not_ using inferential statistics with recorded crime data

:::: {.columns}

::: {.column width="66%"}

![Scottish Government funnal plot of reconvictions y local authority, 2018-2019 cohort](resources/sg-reconviction-funnel-plot-02.png)

https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2021/10/reconviction-rates-scotland-2018-19-offender-cohort/documents/reconviction-rates-scotland-2018-19-offender-cohort/reconviction-rates-scotland-2018-19-offender-cohort/govscot%3Adocument/reconviction-rates-scotland-2018-19-offender-cohort.pdf

:::

::: {.column width="34%"}
"Chart 12 is standardised to take into account some of the differences between
local authority groups attributable to the characteristics of offenders, such as
the number of previous offences, sentence, sex, and age. It provides the
standardised reconviction rates3 against the observed number of offenders
minus expected number of offenders. Since all local authorities groups are
within the funnel it suggests that the apparent differences in reconviction rates in Chart 11 are primarily attributable to either the variation in the
characteristics of the offenders, the type of crime they committed, or the
sentence they received, rather than differences in ‘performance’ between the
local authority groups."

:::

::::

## Reflections on this example

- Even though the reconviction cohort could be thought of as 'full population' data, Scottish Government use ideas from inferential statistics to interpret the data
- I think this well justified
- There are two implied counterfactuals
  - What could things have looked like for this cohort given the "variable outcomes" we would expect "in the normal run of events"?
  - What would reconviction rates have looked like if all local authorities had the same distribution of characteristics of people who offended, convicted for the same offences and receiving the same sentences


## Survey data

- In response to known issues with measuring levels of crime with administrative data, since the 1980s criminologists in some parts of the world have been surveying the public to ask about their levels of victimization.
- Scottish Crime and Justice Survey​
- Crime Survey for England and Wales​
- Equivalents in other countries, primarily in Western Europe, North America and Australasia - Also smaller geographical scale surveys such as The Mayor's Office for Policing And Crime (MOPAC) Survey in London

---

## Survey data: benefits

- Typically ask people about their experiences of victimization in the last year​
- Can measure crime that isn’t reported to the police​
- Usually don’t ask about people’s offending behaviour​
- Good for measuring common crimes, bad for measuring rare crimes

---

## Answering our key questions

- "Why has the data been collected (and collected in this way)?
    - Data collected as part of government reporting on experiences and attitudes of criminal justice
- How has the data been collected and/or by whom or by what?
    - Collected by professional survey agencies
- What/who is included and what/who is excluded?
    - Random sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)
  - For those who agree, one adult (age 16 or over) in each household is randomly selected for interview​
  
## Implications
  
- Therefore, *by design* no:​
  - Children​
  - Homeless people​
  - People living in communal establishments (e.g. students, people in prison)
  - Also in practice may exclude those in 'fragile, disjointed households' [@carr-hillMissingMillionsMeasuring2013]
  - For SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​

But this excludes people, students, people in prison, and those living in refuges. However, “Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”
  
---
  
- What is the context for the data collection (routine activity, bespoke intervention, to meet a target)?
    - Routine reporting
- Has the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?
    - When it gets to you it has usually been validated. Some information is also restricted, such as 'capping' counts of victimization - we'll talk about this later...
- What are the relevant definitions and concepts that govern the data/data collection?"
  - You can read the question wording online! But... the questions included can changes over time (https://www.sccjr.ac.uk/wp-content/uploads/2024/02/group-two_hackathon-output.pdf)
  
  
---
  
## Implications for analysis

- With surveys we almost always do want to do inferential statistics!

- Surveys are very rarely random samples; most come with weights
- Use the weights for descriptives to gross up to national populations
- Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model - it may depend on the specifics of your survey
- But, weights will only help adjust the data you see towards the target population in the survey design frame. Weighting cannot adjust for populations who are excluded from the survey by design.



--- 

# Recap:
# Data provenance determines what types of analysis are appropriate for a given aim
# Recorded crime and victimization surveys exclude some incidents/population groups by design
# There are pros and cons of using inferential statistics with recorded crime data



# Practical

- Describe SOI
- Give examples from my thesis of being inconsistent

- Do I need significance testing here?

- What's the story of *your* crime data?


# Break {background-color="#666666"}


# The general linear model  {background-color="#006938"}


---



## A disclaimer

- If you are taking this course I assume that you have some familiarity with linear/regression models of some description
- Here we focus on two elements of telling stories with GLMs
  - Matching the 'family' to the type of data you have
  - Interpreting coefficients 

---


## A(nother) disclaimer

- You could spend months and months studying GLMs and their various extensions. Some specific flavours of model that we *don't* cover, but which may be useful for your own work include:
- Mixed/multilevel/hierarchical/etc (see e.g. https://www.cmm.bris.ac.uk/lemma/)
- Generalized Additive models (GAMs) (which are less common in criminology, but are very useful; https://noamross.github.io/gams-in-r-course/)
- 'Bespoke' models (very infrequently used in criminology to my knowledge, https://betanalpha.github.io/assets/case_studies/generative_modeling.html, but see e.g. https://josepinasanchez.uk/wp-content/uploads/2018/09/bsc-presentation.pdf)


## Large worlds and small worlds

- A statistical model is only every the map, and never the territory
- Richard McElreath describes the statistical models as representing 'small worlds' of their own - distinct from the 'large world' that we actually live in
- Our spreadsheets and model coefficients can only summarize the small world for us, and necessarily omit some of the complexity of the large world - [this is a good thing]((https://kwarc.info/teaching/TDM/Borges.pdf)!
- In this session we'll overview the most common[^3] way in which criminologists understand the 'small world' of their spreadsheets: the general linear model (GLM)
- But be warned, as soon as we want to understand the large world we can run in to problems if all we focus on are the coefficients from the small world (we'll talk about this later)

---


## Anatomy of a GLM

-   Stochastic (or random) component
-   Systematic component
-   Link function that converts between the parameter estimates and the form of the outcome (we'll say more about this later)

## Anatomy of a GLM

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## The linear model

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

## Count data

Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims. 
(Whilst we may see crime data be re-expressed as *rates* per 1,000 population, before this they are counts.)

## Poisson model

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The coefficients from Poisson models (which range from $-\infty$ to $\infty$) are converted to be predicted counts that are non-negative integers via the $\log$ link function. This lets us have coefficients which can take any value (-2! 0.5!), but then convert these to counts as our count outcome demands.

## why not just use a linear model?

- If you really want to you can just use a linear model for count data. This will still give you an accurate model of the mean of your outcome. However, there are two main problems with this approach.
- But this is at odds with the 'story' of our data - that we know counts have to be positive integers - you can't experience -1 victimization incidents, or record six and a half homicides

## Negative counts?

First, if you have small counts (say, if you were modelling homicides in Scotland by Local Authority) the uncertainty in the mean estimate may give you a confidence interval below zero:

```{r}
library(dplyr)

set.seed(12346)

n_draws <- 1e3

dat <- 
data.frame(
  y = rpois(n = n_draws,
            lambda = 0.001) # draw from poisson distribution with mean 0.001
)

dat |> 
  count(y)

lm(y ~ 1, data = dat) |> # fit intercept-only model with normal outcome
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error)


glm(y ~ 1, # fit intercept only model with poisson outcome
    family = "poisson",
    data = dat) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low))

```

Here the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive). 


## Non-constant variance?

Second, the standard linear model assumes constant variance. But in practice we probably want more variance for larger counts. 

(Figure from @roback2021beyond)

!["Figure 4.1: Regression models: Linear regression (left) and Poisson regression (right) from Roback"](https://bookdown.org/roback/bookdown-BeyondMLR/bookdown-BeyondMLR_files/figure-html/OLSpois-1.png)



## Interpretting coefficients

- Per UCLA OARC: "for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant."

- The most straightforward way to interpret coefficients from Poisson models is to exponentiate the coefficient value. This converts the beta coefficient into an Incident Rate Ratio (https://stats.oarc.ucla.edu/stata/output/poisson-regression/)


## Good news and bad news

- To recap: count data are common in criminology
- The standard linear regression models can run into problems with count data (this is bad)
- Generalized linear models (such as the poisson model) don't have the same problems (this is good!)
- ... but GLMs have their own problems (this is bad)

## Problems with Poisson: over-dispersion

- This is quite technical, so I will be brief!
-   poisson assumes mean and variance are the same (in that there is only one term in the model, $\lambda$, which describes both the average and the variability around the average)

- This is quite a brittle assumption though, and there's no reason that real-life data has to obey it. So what? If your data are over-dispersed (the conditional variance exceeds the conditional mean), you will get standard errors that are too small.
- p-values are too optimistic
- need some way to account for over-dispersion

---

## Solutions?

### What do to about over-dispersion

- If all you care about is your standard error being too small you can use quasi-poisson
- This gives same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it's also faster than the standard alternative...
- The negative binomial is a flavour of statistical model ^[okay, strictly speaking there are different varieties of negative binomial model. So... flavours? [@hilbeModelingCountData2014]] for count data which has an extra "dispersion" parameter, which allows you to model the over-dispersion directly
- People disagree as to which is preferable



- Example of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf

---

##

Also - rates!

## Practical

In this practical we'll fit some count models in R

# Morning recap {background-color="#006938"}

## Morning recap

- The story of how our data came to be influences how we should analyse it
- GLMs are the most popular way to tell stories about the `small world` of our data


# Lunch :sandwich: {background-color="#666666"}

---



# Measurement error, selection and confounding  {background-color="#006938"}


---

## Going from the smal world to the large world

- In the last session we talked about GLMs as 'small worlds' which summarise the data in our spreadsheets
- There are lots of reasons we may be skeptical about applying conclusions drawn from 'small world' of our data and model to the 'large world' we live in
- In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models
- Again, books and books have been written about each topic, so this will only offer a brief overview of each, with links to further reading for those who want to know more.

---

## Are coefficients sufficient?

- We should, I think, be skeptical about the numbers in our spreadsheets and the coefficients that summarise them, by default [@greenlandIntervalEstimationSimulation2004]
- The coefficients from your model will accurately reflect the 'small world' of your dataset, but they may not reflect the 'large world' that we live in
- Coefficients from GLMs may:
  - be (predictably) inaccurate (measurement error);
  - omit some variables that we theoretically think are important (confounding), or;
  - omit some people/cases we care about (selection effects)
- These are problems that are _hard, if not impossible, to solve based on the data we observe alone_
- There is a large literature in epidemiology study these issues called __Quantitative Bias Analysis__
- 


# Act One: Measurement error {background-color="#77BF22"}


## Measurement error

- Measurement error is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet
- In practice we know criminological data are likely to be measured with some degree of error
- For example, police recorded crime data is not a perfect measure of the amount of crime 'out there' in society
- As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.


---

## Measurement error in practice: police recorded crime

-   From our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data.
-   So if we are interested in, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an *under* count.
-   The many years of work comparing crimes recorded to the police with victimization surveys - the 'dark figure of crime' - attests to this.

## Measurement error in recorded crime

- Measurement error in recorded crime is _proportional to the amount of crime_ (Pina Sanchez et al 2023)
- There is a larger gap between the amount of crime recorded and 'true' crime levels in areas with large numbers of recorded crimes compared to areas with very low recorded crime


- Measurement error in crime due to e.g. lack of willingness to report may itself be related to other variables in your analysis (e.g. economic inequality)
- This is called 'differential' measurement error

## `rcme`

Pina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.

> Work through their example here? https://osf.io/preprints/socarxiv/sbc8w

Open questions - what about survey data? Models other than linear models? What about measurement error in independent variables?

- At the moment seems like this focuses on measurement error with recorded crime as the dependent variable

---

## Example Two: Measurement error in victimizaton survey data

-   Historically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete
- In 2019, ONS said that "Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way." [@officefornationalstatisticsImprovingVictimisationEstimates2019]

-   However, it also means that people who experienced more than five incidents of a particular 'series' crime type did not have their data accurately recorded

## Impact of capping on analysis

-   This was particularly important for women's reporting of violent victimization [@walbyViolentCrimeIncreasing2016] - women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year. A second measurement error issue came from the '97 code' - the option to report the number of incidents experienced as '96/too many to count'. Instead, based on the domestic violence literature Walby and colleagues propose using an estimate of 60 incidents for those who report the 97 code.

- This would bias estimates of total crimes , but not prevalence (the number of victims)

-   Sometimes it's possible to use uncapped data

## (Bonus) Example Three: Ethnic disparities in Covid Fines

- https://blogs.ed.ac.uk/edinburghlawschool/wp-content/uploads/sites/8261/2023/03/NPCC-Report-March-2023-final-1.pdf
- Wanted to calculate disparities in Covid fines issued across England and Wales to people from different ethnic backgrounds
- Covid fines data were from 2020-2021, but the most recent data on ethnicity by Police Force Area was from 2016
- "This may mean that the FPN rates by
ethnicity – and so our calculated disparity rates – are inaccurate due to changes in the population denominator used to calculate the rates. This could particularly affect
PFAs with small ethnic minority populations, where small changes in the estimated
ethnic minority population could lead to large differences in the estimated FPN rates."



---

## Adventures in measurement error

- The two examples we've looked at have focused on measurement error in the outcome (recorded crime and victimization)
- But it also matters where in your model the measurement error manifests (/manifests more)
- If you have error in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results (meaning that they are less likely to be statistically significant).
- Measurement error in your key independent variable may bias your regression coefficients *downwards*, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. 
- But if there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to 'under controlling' - and finding statistically significant coefficients where there are none. 
- It's quite context dependent - so you need to know the likely source of error in your specific dataset
- Other than in simple scenarios *we may not know what impact it is having*. Uh oh!




# Act Two: Selection effects

## Selection effects

Selection bias arises when there are people who we would have liked to observe in our study but we don't observe them, and this lack of observation is related to their characteristics. Put another way, we can think of selection bias as affecting the rows of our dataset - there are some rows that are missing that we would like to see.

[@greenlandSensitivityAnalysisBias2014]


## An example of selection bias


Imagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.

Police collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?

The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”

In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset *would* know this.

Even if we *do* know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we'd need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)

We're going to hear a lot about 'strong and untestable assumptions'.

## Uh oh

“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”

\[INSERT FIGURE 2 FROM KNOX\]

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.

Others [@gaeblerCausalFrameworkObservational2022]suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).

## Solutions?

- @knoxAdministrativeRecordsMask2020a suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify *causal* relationships *there is no general solution* that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records.
- The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.

---


# Act Three: Confounding

## Residual or unmeasured confounding

- Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we _don't have a measure for this_
- Famous example of smoking and lung cancer
  - RA Fisher argued that the strong association between smoking and lung cancer "could both be influenced by a [unmeasured confounder, such as] constitutional make-up, perhaps genetic in origin, which predisposes individuals to both of them"
  - Cornfield and colleagues demonstrated that such a confounder would have to be implausibly large to produce the observed association, and so was implausible
- (This matters most if you are using your analysis to make a causal claim)

## Residual or unmeasured confounding
- There are lots of ways you can try to quantify the potential impact of unmeasured confounding which are discussed in epidemiology, in the sub-field of [quantitative bias analysis](https://link.springer.com/book/10.1007/978-3-030-82673-4)
- I have not seen these methods used in criminology very much (this is a shame!)
- One informal approach to unmeasured confounding is just to claim that the measures you _do_ have are sufficient...

## Talking about residual confounding


[ add screenshots ]

Probably the biggest limitation of our study is that the IPTW modeling
approach we adopted assumes no unmeasured covariates linked to both
treatment and outcome. In practice, the criterion of having no unobserved
confounding is impossible to verify—the data in any observational study
provide no definitive information (Robins, Hernán, and Brumback, 2000).
As discussed above, however, we tried to counteract this limitation by
exploiting what we believe are rich individual baseline data and timevarying covariates over the full life course in order to model the
propensity to marriage. It is hard to imagine what the missing time-stable
or time-invariant covariates are that would overcome the magnitude and
robustness of results. From IQ to the cumulative history of both the
outcome and the treatment, we accounted for 20 baseline covariates and
approximately a dozen time-varying confounders measured from widely
varying sources—many of which predict the course of marriage as
theoretically expected (table 1). 



We thus argue that omitted confounders would have to be implausibly
large to overturn the basic results obtained under a number of different
model specifications and assumptions.22

22 A formal sensitivity analysis (see Robins, 1999: 167–73) is beyond the scope of the
current paper. Moreover, such analyses require assumptions about the magnitude,
direction, and functional form of potential biases that ultimately raise more
questions than they answer

(https://psycnet.apa.org/record/2008-07491-001)

## Is this a good justification?

- It's true that you have to make assumptions about the unmeasured confounding to know the impact on your results, and so it's necessarily speculative

- But this is possible!

"However, increasing the number of covariates is hardly
a persuasive approach to ruling out potentially important confounders,
as it is unlikely that one can adequately measure all such confounders"


## Should I do a quantitative bias analysis?

- After all this talk... not necessarily!
- Epidemiologists who work in this area suggest you only need to bother with this stuff if you 'claim to offer near-definitive conclusions' based on your study
- So the key thing is how we talk about our models - it is *us* that moves between the stories in the 'small world' of our model to the 'large world'
- If you do want to apply a technical fix, the specifics will depend on the problem you are interested in and the data you have (see Knox et al.)
- 

# Recap {background-color="#006938"}

## Recap

- If you are working with observational data (as is common in criminology), coefficients from a GLM may well be biased
- Quantitative bias analysis describes three secnarios where we may want to be critical about our results:
    - Measurement error
    - Selection effects
    - Unmeasured confounding
- Measurement error has received attention in criminology when analysis recorded crime and victimization counts
- So has selection effects in police data on stops and when analysing court data
- Unmeasured confounding less so
- The potential impact of these three factors depends on your specific data and analysis - there are no general solutions :shrug:

# Practical {background-color="#006938"}

- Using either the generative story that you came up with in Session One, or the description of the SOI from the first practical, describe the possible effects of:

    -   Measurement error?
    -   Selection effects?
    -   Unmeasured confounding?

Are you concerned about all these? Some more than others? Are there obvious steps you could take to address them?

# Break {background-color="#666666"}

---


# Turning coefficients into meaningful quantities of interest: Simulation {background-color="#006938"}


---

## Simulation

- In the last session we spent time being skeptical about our model coefficients, and looked at some methods which adjust coefficients to account for possible biases in the data.
- Now we are going to translate coefficients into more interesting and informative quantities. This is a great way to make results more informative and accessible [@kingMakingMostStatistical2000].



## Translating coefficients

- In some simple (linear) models you read off a coefficient directly as the quantities that we are interested in
- In more complex models, such as generalized linear models, we often want to convert the coefficients to express results in a more accessible way
- In poisson regression the model coefficients are also commonly expressed as rate ratios, by exponentiating the coefficients (we did this already in the practical!)


## Translating coefficients: challenges

- These approaches don't scale well with more complicated models, or complicated transformations (McElreath; Gelman and Pardoe)
- A common way to translate model coefficients into more meaningful quantities is to use all the coefficients in the model to calculate predicted values of the outcome
- These are often called ['marginal effects'](https://marginaleffects.com/) (although this terminology is confusing)
- The terminology is confusing in part because there are lots of related but subtly different quantities people call marginal effects (see https://marginaleffects.com/vignettes/get_started.html)
- As a result, I like Gelman and Pardoe's terminology of 'adjusted predictive comparisons'



## Calculating a predicted values from models

- In the second practical we fit a GLM describing the relationship between violent crime and deprivation which gave us the results
- Intercept = 2.4573632, simd 2020 rank = -0.0003029

- Using our GLM formula, we know that the log of the expected violent crime for a data zone with an SIMD rank of 0 is 2.4573632
- NB no such datazone will exist by definition, but that's okay!
- We can calculate the expected violent crime incidents by exponentiating the intercept, giving us 11.674 violent crimes
- To calculate violent crime in the most deprived datazone in Scotland, we just add 1 * -0.0003029 (the coefficient for SIMD) to 2.4573632, and then exponentiate: 11.670
- To calculate violent crime in the least deprived datazone in Scotland, we add 6976 * -0.0003029 (the coefficient for SIMD) to 2.4573632, and then exponentiate: 1.41

## Calculating predictions

:::: {.columns}

::: {.column width="66%"}


```{r preds}
library(ggplot2)

data.frame(
  simd = seq(1, 6976)
) |> 
  mutate(pred_vio = exp(2.4573632 + simd * -0.0003029)) |> 
  ggplot(aes(x = simd, y = pred_vio)) +
  geom_line()



```


:::

::: {.column width="34%"}
In fact, we can calculate these predictions across the whole range of SIMD

:::

::::



## Uncertainty
- A key challenge is propagating the appropriate uncertainty in our results - especially in more complicated models when predictions might be a function of multiple parameters
- We can use simulation to propagate this uncertainty - and we can apply this approach to any generalized linear model and any Derived Quantity of Interest (DQI) - be it a marginal effect or something else
- The simulation process is described by [@kingMakingMostStatistical2000] and implemented in the R package clarify (https://github.com/iqss/clarify)
- This approach propagates both the uncertainty in the model's coefficients, and the correlations between the coefficients (for technical reasons that are beyond me, this is important - see Gayle and Lambert 2007 https://journals.sagepub.com/doi/10.1177/0038038507084830)





---

# But first {background-color="#006938"}


---

## Simulating random variables: a brief introduction

:::: {.columns}

::: {.column width="66%"}

```{r rnorm_sim_v1}
library(tibble)
library(ggplot2)

var1 <- 
rnorm(
  n = 10000,
  mean = 0,
  sd = 1
)

tibble(
  var1 = var1,
) |> 
  ggplot(aes(x = var1)) +
  geom_histogram()

```

:::

::: {.column width="34%"}
`rnorm()` lets you simulate normally-distributed random variables.

Here we simulate a normally distributed random variable and plot its distribution
:::

::::

---

## Simulating a second variable


:::: {.columns}

::: {.column width="66%"}

``` {r rnorm_sim_v2}
var2 <- 
rnorm(
  n = 10000,
  mean = 1,
  sd = 1
)


tibble(
  var1 = var1,
  var2 = var2
) |> 
  ggplot(aes(x = var1, y = var2)) +
  geom_point() +
  geom_density_2d()


```

:::

::: {.column width="34%"}
Now we add a second variable and plot them together.

We can see that they are uncorrelated (as we would expect)
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}


```{r mvtnorm_sim}
cor_data <- MASS::mvrnorm(
  n = 10000,
  mu = c(0, 0), # mu instead of mean
  Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2, ncol = 2) # Sigma instead of sd
) |> 
  as.data.frame()

cor_data |> 
  tidyr::pivot_longer(cols = c(V1, V2),
                      names_to = "variable",
               values_to = "value") |> 
  ggplot(aes(x = value)) +
  facet_wrap(~ variable) +
  geom_histogram()

```


:::

::: {.column width="34%"}
If we use `mvrnorm()` the resulting simulations can be correlated. Here I set a correlation of 0.9.

`V1` and `V2` are both normally distributed...
:::

::::

---

## Simulating correlated variables


:::: {.columns}

::: {.column width="66%"}

```{r cor_graph_2}
cor_data |> 
  ggplot(aes(x = V1, y = V2)) +
  geom_point() +
  geom_density_2d()

```


:::

::: {.column width="34%"}
... but highly correlated. 
:::

::::


---

## Back to our example

:::: {.columns}

::: {.column width="66%"}

``` {r clarify}

simd <- readRDS(url("https://github.com/benmatthewsed/statistical-methods-criminology-slides/raw/master/resources/simd_crime_sim.rds"))


mod_vio_simd_qp <- glm(vio_integer_sim ~ simd2020_rank, 
                      family = "quasipoisson",
                      data = simd)


```


:::

::: {.column width="34%"}
If we fit the model from the second practical again...
:::

::::

---

## Back to our example


:::: {.columns}

::: {.column width="66%"}


``` {r clarify2}

library(clarify)

sims <- sim(mod_vio_simd_qp, n = 1000) 

sim_res <- sim_setx(sims, x = list(simd2020_rank = seq(1:6976)))

plot(sim_res)


```

:::

::: {.column width="34%"}
We can now get confidence intervals around our predictions.

This approach can also be used when we want to look at the effects of multiple variables, and where we want to describe more complex transformations of coefficients...
:::

::::


---

## An advanced example: victimization divides

- @hunterEquityJusticeCrime2016 wanted to describe how victimization inequality had changed over time
- They used the results of a fitted regression model to calculate a measure they call 'Victimization Divide'
- This measure is defined as:

(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)

---

## Victimization divide

This is analogous to exploring the percentage change in victimization inequality between two comparison years.

---

## Victimization divides

To calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a GLM to predict the number of burglary victimization incidents experienced by households in England and Wales in 1993 compared to 20089. They use the coefficients from these models as inputs into the Victimization Divide formula. 

---

## Victimization divides

- Based on this analysis @hunterEquityJusticeCrime2016 conclude that burglary victimization inequality had increased for:
  -   single adult households compared to other households
  -   social renters compared to owner occupiers
  -   households without a car compared to those with one car
  -   households leaving their home unoccupied any amount of time on a typical weekday compared to those never leaving the home
  -   households in areas without neighbourhood watch compared to those with the scheme
  -   households earning at least £50,000 per annum compared to those on a £10,000–£29,999 annual income
  -   inner city residents compared to households in rural areas

---

## Victimization divides

- But this analysis used data from the Crime Survey for England and Wales
- So, in line with the generative story of these data, we want to assess the uncertainty in these estimates which come from projecting from sample to population
- We can do this with the King et al. simulation approach


---

## Simulating Victimization Divides

- In this process we:
- Use a fitted regression model to simulate a set of coefficient values from the regression model's variance-covariance matrix (usually at least 1000 draws)
- Calculate the VD for each one these simulated coefficient draws


---

## Simulating DQIs: worked example

:::: {.columns}

::: {.column width="66%"}
```{r vid_fn, fig.show = 'hide'}
#| echo: true

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```
:::

::: {.column width="34%"}
This is what the VD formula looks like in `R`
:::

::::

---

## Simulating DQIs: Getting ready

:::: {.columns}

::: {.column width="66%"}

```{r vd_prep}
#| echo: true

# load packages

library(MASS)
library(tidyverse)

# reading in data

dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )

# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```
:::

::: {.column width="34%"}
These are data from the Crime Survey for England and Wales showing victimization for men and women in 2015 and 2020. You can find more info on the data and approach [here](https://osf.io/k8j9e/)
:::

::::

---

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

```{r vd_models}
#| echo: true

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))
```

:::

::: {.column width="34%"}
We can fit a simple regression model to the data in each year to calculate the log-odds of being a victim for men and women. In this example I fit a separate model for 2015 and 2020.
:::

::::

--- 

## Calculating VDs

:::: {.columns}

::: {.column width="66%"}

Model1 :
```{r vd_ests_2015}
#| echo: true
results_2015 <- 
broom::tidy(model2015) |> 
  mutate(est = exp(estimate))

results_2015
```

Model 2:
```{r vd_ests_2020}
#| echo: true

results_2020 <- 
broom::tidy(model2020) |> 
  mutate(est = exp(estimate))

results_2020

```


:::

::: {.column width="34%"}
Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having __11%__ greater odds of being a victim of crime. In contrast, Model 2 finds that men had a __5%__ greater odds of being a victim of crime than women in 2020 - however this difference does not meet the 95% threshold for statistical significance.
:::

::::

---

## How much has victimization inequality changed?


:::: {.columns}

::: {.column width="66%"}

```{r vd_calc}
#| echo: true
victim_divide(
  base_y1 = results_2015$est[[2]],
  base_y2 = results_2020$est[[2]]
  )
```


:::

::: {.column width="34%"}
Calculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by __52%__ between the two years. 

Based on the point estimates from the model, we would conclude that victimiztion inequality between men and women fell by more than half!
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim}
#| echo: true
# set seed
set.seed(nchar("vict divide") ^ 4)

n_sims <- 1e4

draws_2020 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2020),
    Sigma = vcov(model2020)
  )

draws_2015 <-
  MASS::mvrnorm(
    n = n_sims,
    mu = coef(model2015),
    Sigma = vcov(model2015)
  )
```


:::

::: {.column width="34%"}
We can pass the models' coefficients and variance-covariance matrices to `mvrnorm` from the `{MASS}` library.

This gives us a set of 10,000 coefficients for each model.
:::

::::


---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_calc}
#| echo: true
draws_2015 <-
  draws_2015 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

draws_2020 <-
  draws_2020 |>
  as.data.frame() |>
  as_tibble() |>
  mutate(est = exp(`fct_rev(sex)men`))

# combine the results
sim_dat <-
  tibble(
    vd = victim_divide(base_y1 = draws_2015$est,
                       base_y2 = draws_2020$est)
  )


```


:::

::: {.column width="34%"}
Once we tidy the results up a bit, we can pass these simulated draws to our `victim_divide()` function...
:::

::::

---

## How much has victimization inequality changed?

:::: {.columns}

::: {.column width="66%"}

```{r vd_sim_res}
#| echo: true
sim_dat |> 
  reframe(
    vds = quantile(vd, c(0.025, 0.5, 0.975)),
    centile = c("2.5%", "50%", "97.5%")
    )


sim_dat |> 
  ggplot(aes(x = vd)) +
  geom_histogram(binwidth = 0.1) +
  scale_x_continuous(limits = c(-2, 2)) +
  geom_vline(aes(xintercept = 0))

```


:::

::: {.column width="34%"}
We can see the 2.5 and 97.5 percentiles of the simulated VDs, as well as the histogram of their distribution. We can use these intervals to approximate a statistical significance test - if the interval includes zero the coefficient is not 'statistically significant' at the standard level.

These intervals include zero, so the apparent 52% reduction in victimization inequality would not be 'statistically significant' at the standard level.
:::

::::

---

## Simulation is great

- The beauty of this simulation approach is that it generalizes to __any DQI__ that is a function of the model's parameters, such as VD, and to __any GLM__ [@kingMakingMostStatistical2000]
- Say that instead of the VD were interested in the absolute difference in predicted victimization rates after controlling for other factors (like a marginal effect)
- Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating the model's uncertainty into our estimates

---

## Simulation in practice

- In the second example we did the simulation by hand. This is useful as a way to understand the method, but whilst it's good to know how this works, in practice there are R packages which can do this for us. 
- One good option is [clarify](https://cran.r-project.org/web/packages/clarify/index.html), as used in the first example
- There is inherent uncertainty in simulation results, so it's crucial to set the seed for the random number generator so you get the same results every time. 
- You also need to run enough simulation draws to get a good estimate of the uncertainty. The default for clarify is 1000 draws.
- More simulations are always better, but will take longer - so there is a pragmatic aspect to how much time and computer resource you have to run simulations.
- In general I would recommend using a package like clarify if you conduct this kind of simulation. It's likely that professionally developed and maintained software will be less error-prone and more efficient than writing your own!

---

## Practical limitations and alternatives

- Simulations can be very time consuming! Especially if your model is complicated and you are making predictions for a lot of cases
- There are other ways to do describe this uncertainty:
  - The ['delta method'])(https://marginaleffects.com/vignettes/uncertainty.html#delta-method) uses calculation, not simulation, but relies on a Normal approximation which may not hold (https://iqss.github.io/clarify/#introduction). This is the default method used by the [marginaleffects](https://marginaleffects.com/) package

---

## Conceptual limitations

- Simulation expresses the uncertainty captured _by our model parameters_
- But remember that our model is wrong!! (Greenland)
- Our model still makes a load of assumptions - basically that we've fit the right model and that we had the right data.
- As we discussed in the previous section, we might also want to be skeptical of our model parameters themselves if we are worried about bias


---

## Practical


# Break {background-color="#666666"}

---

# Ethics  {background-color="#006938"}

## General ethical principles in social science research

- In research ethics and governance there is a lot of discussion about informed consent, not disclosing personal information and so on.
- Also specifically in quantitative work, ethical issues around workflow and 'questionable research practices'
- This is not a general treatment of ethics in social science research
- We're going to focus on the ethics of framing our research and how we present our results

## Framing: The tyranny of the means

- The typical focus of regression results is group average effect
- Even if we have a statistically significant difference between two groups (say, people who live in high deprivation areas versus other areas) in the count of offences recorded by the police, on its own this doesn't necessarily tell us about how likely any individual member of those groups is to offend
- Focusing only on group averages can imply that all members of the group are the same

- Causal quartets?

## Causal quartets

![From @gelmanCausalQuartetsDifferent2023](images/causal-quartets.png){width="90%"}

## Causal quartets

- So what? [@gelmanCausalQuartetsDifferent2023]
- People with the same observable characteristics of your independent variables may have very different values of the outcome
- "Variation among people is relevant to policy (for example, personalized medicine) and understanding (for example in psychology)"
- "Variation across situations is relevant when deciding what “flavor” of treatment to do, for example with dosing in pharmacology or treatment levels in traditional agricultural experiments."


## Same observables, different outcomes

![](images/qlp-results-figure.png){fig-alt="Estimated Adult Conviction Trajectory Probabilities by Sex, Indigenous Status and Childhood System Contact."}

## How we described the figure

"Our results show substantial variation in adult conviction trajectories within sex and Indigenous status groups for people with identical observable characteristics, as well as between our four demographic groups... Even though we see a strong association between Indigenous status, sex and membership of the High/Persistent conviction trajectory (see Appendix 4), it would be wrong to assume that all Indigenous men with multiple childhood system contact end up with High/Persistent adult conviction trajectories—our model estimates that almost half of Indigenous men would _not_ end up with a high-rate, persistent adult conviction trajectory even with this substantial childhood adversity."

https://link.springer.com/article/10.1007/s40865-022-00204-z


## Framing: the importance of language

More conceptually, it's important to think about how we frame the results of any analysis.

Three visualizations from [Data Feminism](https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)

-   language use

-   providing necessary context?

-   deficit narrative

## Stereotyping

"...a narrative that reduces a social group to negative stereotypes and fails to portray them with creativity and agency." (https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)

https://data-feminism.mitpress.mit.edu/pub/czq9dfs5#ndayi2fa1pk

-   the description of your charts is theoretically informed

The value of this exercise is not to say that any of these framings is 'right' (although for the reasons Klein and D'Ignazio outline we might find some preferable to others), but that they reflect different theoretical positions, and give different emphases to contextual factors - factors outside our datasets.

## Example one: Deficit framing

- focus on problems of minority/minoritized groups in reference to advantaged/majority groups
- rather than, say, the advantage experienced by majority groups due to majority group status


## Framing

- Study of (among other things) rates of mental health diagnosis in prisons in New York, broken down by ethnicity (Kaba et al. 2015)
- People from Black or Hispanic backgrounds may be more likely to be seen as criminal rather than experiencing mental ill-health, and so may be less likely to be given a mental health diagnosis
- D'Ignazo and Klein (2020) illustrate ways in which one of the findings from this study could be framed:

## Framing

![](resources/data-feminism-fig-1.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Framing


"The study that produced these numbers contains convincing evidence that we should distrust diagnosis numbers due to racial and ethnic discrimination. The first chart does not simply fail to communicate that but also actively undermines that main finding of the research."


## Framing


![](resources/data-feminism-fig-2.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Which is preferable?

- Is this a deficit narrative?
- "By naming racism and then talking about people of color in the title, the graphic reinforces the idea that race is an issue for people of color only."


## Framing


![](resources/data-feminism-fig-3.jpeg){fig-alt="Portrayals of the same data analysis. Images from Data Feminism (2020) by Catherine D'Ignazio. Data from Fatos Kaba et al. 'Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental Health Service'."}

## Framing

- Third framing focuses on advantages of White group
- The subtitle better fits the researchers' conclusions



## What are our ethical responsibilities

"Placing numbers in context and naming racism or sexism when it is present in those numbers should be a requirement—not only for feminist data communication, but for data communication full stop."

Data Feminism is mostly aimed towards data scientists and data journalists, not academics. Does this change how we should view their recommendations?

(the original study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4539829/)

- Our analysis exists in a context of (Tilley) durable inequalities




## Example two: community loss

- @simesPunishingPlacesGeography2021 gives a good example of how we may want to come up with theoretically informed measures, or theoretically re-frame measures. Simes analysed imprisonment data from the state of Massachusets in the USA, including spatial regression of prison admission rates and how these relate to "racial demographics, social and economic disadvantage, arrest rates, and violent crime" [@simesPlacePunishmentSpatial2018a].

- As part of this analysis Simes suggests that the cumulative years sentenced to residents of a particular neighbourhood be thought of as 'community loss'.


## Example two: community loss

- Despite being derived from data on individual punishment histories, this conceptualization reflects the chronic and long-term exposure to loss due to imprisonment in different neighbourhoods. 
- This terminology highlights the effects of imprisonment on the communities in which people who end up in prison lived prior to their imprisonment, rather than just focusing on the people who are currently in prison.


## Language and framing

- The language we use may depend on the audience and the aims of our analysis
- I might desribe a figure differently in an academic journal article versus writing for a general audience

---

# Practical





# Overview  {background-color="#006938"}

## Overview

- Whew! We've covered a lot of ground today
- The key idea of this workshop is that the story of how our data came to be ('data provenance') should influence our analysis and the story we tell about our results
- We looked at common data provenance stories of police recorded crime and victimization surveys
- We talked about GLMs as a common way of telling stories about the small world of our data
- We looked at how we might want to critique the stories our GLMs are telling, using tools from quantitative bias analysis
- And how to use simulation approaches to tell more meaningful stories from our results
- We discussed some ethical issues that come up when we present our results

## Where next?

- Data provenance: Over to you!
- GLMs (and beyond)
  - (Statistical Rethinking)[https://github.com/rmcelreath/stat_rethinking_2024]
  - (Quantitative Social Science Methods, I (Gov2001 at Harvard), Gary King)[https://www.youtube.com/watch?v=qs2uCuDL2OQ&list=PL0n492lUg2sgSevEQ3bLilGbFph4l92gH]

- Quantitative Bias Analysis: :shrug: Maybe (_Quantitative Assessment of Systematic Bias: A Guide for Researchers_)[https://journals.sagepub.com/doi/10.1177/00220345231193314] or [@kawabataQuantitativeBiasAnalysis2023]?
- Simulating meaningful quantities from results: (`{clarify}`)[https://iqss.github.io/clarify/] and (`{marginaleffects}`)[https://marginaleffects.com/]
- Ethics: (Data Feminism)[https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3]

---

## References